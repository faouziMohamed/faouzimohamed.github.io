<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="utf-8">
    <title>R√©seaux de neurones</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name='author' content="Faouzi Mohamed">
    <meta name="color-scheme" content="dark light">
    <link rel="stylesheet" type="text/css" href="../css/style.css">
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {fontCache: 'global'}
        };
    </script>
    <!--<script id="MathJax-script" async src="../mathjax/es5/tex-svg-full.js"></script>-->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <noscript>
        <link rel="stylesheet" href="../css/noscript.css" />
    </noscript>
</head>

<body>
    <nav id='header-nav' id="top">
        <div id="menu">
            <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                xml:space="preserve" width="32" height='32'>
                <g>
                    <path d="M896.6,291.5H103.5c-51.5,0-93.5-41.9-93.5-93.5c0-51.5,
                        41.9-93.5,93.5-93.5h793.1c51.5,0,93.4,41.9,93.4,93.5C990,249.5,
                        948.1,291.5,896.6,291.5z M103.5,130.8c-37.1,0-67.2,30.2-67.2,
                        67.2s30.2,67.2,67.2,67.2h793.1c37,0,67.2-30.2,
                        67.2-67.2s-30.1-67.2-67.2-67.2H103.5z" />
                    <path d="M733.7,593.5H103.5C51.9,593.5,10,551.5,10,500c0-51.5,
                        41.9-93.4,93.5-93.4h630.2c51.5,0,93.4,41.9,93.4,93.4C827.1,551.5,
                        785.2,593.5,733.7,593.5z M103.5,432.8c-37.1,0-67.2,30.2-67.2,67.2c0,
                        37.1,30.2,67.2,67.2,67.2h630.2c37.1,0,67.2-30.2,
                        67.2-67.2c0-37-30.1-67.2-67.2-67.2H103.5z" />
                    <path d="M570.9,895.4H103.5c-51.5,0-93.5-41.9-93.5-93.4c0-51.5,41.9-93.5,
                        93.5-93.5h467.4c51.5,0.1,93.4,42,93.4,93.5C664.3,853.6,622.4,895.4,570.9,
                        895.4z M103.5,734.8c-37.1,0-67.2,30.2-67.2,67.2c0,37,30.2,67.1,67.2,67.1h467.4c37,
                        0,67.2-30.1,67.2-67.1s-30.1-67.2-67.2-67.2H103.5z" />
                </g>
            </svg>
        </div>
        <ul id="main-list">
            <li>
                <a href="../index.html">
                    <svg id="home" version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000"
                        enable-background="new 0 0 1000 1000" xml:space="preserve" width="25" height="25">
                        <g>
                            <path d="M834.6,812.2V413.9L488.3,253.7L140.7,422.6v389.6c0,47.8,19.7,86.6,
                            43.9,86.6h43.9V634.5c0-9.5,7.7-17.1,17.1-17.1h151.9c9.5,0,17.1,7.7,17.1,
                            17.1v264.3h333.4C795.9,898.8,834.6,860,834.6,812.2z M624.7,786.1c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V786.1z M624.7,683.4c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V683.4z M727.8,786.1c0,9.5-7.7,17.1-17.1,
                            17.1h-47.2c-9.5,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,
                            17.1,7.7,17.1,17.1V786.1z M727.8,683.4c0,9.5-7.7,17.1-17.1,17.1h-47.2c-9.5,
                            0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,
                            17.1V683.4z" />
                            <path d="M960.6,317.6l-126-59.6v-44c0-32.8-23-59.3-51.4-59.3c-27,0-49,24.2-51,
                            54.9L513.8,106.1c-14.2-6.7-30.6-6.6-44.6,0.3L38.7,318c-25.5,12.5-36,43.3-23.5,
                            68.7c12.5,25.4,43.3,35.9,68.7,23.5l408.3-200.6l424.4,200.9c7.1,3.4,14.6,4.9,22,
                            4.9c19.2,0,37.7-10.8,46.4-29.4C997.2,360.4,986.2,329.8,960.6,317.6z" />
                        </g>
                    </svg>
                    <span class="li-label">Acceuil</span>
                </a>
            </li>
            <li>
                <a href="contexte.html">
                    <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                        xml:space="preserve" width="32" height="32">
                        <g>
                            <g transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                                <path d="M290.7,3020.7c-83.7-36.8-169.5-140.9-183.8-222.6c-6.1-36.8-8.2-1270.4-6.1-2744.9c6.1-2669.4,
                                6.1-2679.6,49-2734.7c22.5-30.6,67.4-75.6,98-98l55.1-42.9l3986.7-6.1c2193.5-2.1,
                                4015.3,0,4052,6.1c145,26.6,161.3,53.1,876.2,1484.8C9767.2-236.8,9900,43,9900,
                                108.3c0,65.4-132.8,345.2-682.1,1444c-375.8,751.6-702.6,1384.7-729.1,1409.2c-26.6,
                                24.5-75.6,53.1-106.2,65.4c-40.8,16.3-1268.3,22.5-4043.9,22.5C1070.9,3047.3,341.8,
                                3043.2,290.7,3020.7z M8633.7,
                                1252.1l571.9-1143.7l-571.9-1143.7l-571.9-1143.7H4406.1H750.2V108.3v2287.4h3655.8h3655.8L8633.7,
                                1252.1z" />
                                <path d="M7042.7,1056C6526,917.1,6205.4,380,6336.1-126.5c136.8-520.8,674-847.6,1180.5-714.8c181.8,
                                47,318.6,126.6,455.4,263.5c251.2,251.2,349.2,590.2,263.5,917C8098.6,868.1,7555.4,1194.9,7042.7,
                                1056z M7414.4,406.5c210.4-87.8,261.4-361.5,98-524.9c-132.8-132.8-320.6-132.8-453.4,0C6805.8,132.9,
                                7085.6,545.4,7414.4,406.5z" />
                            </g>
                        </g>
                    </svg>
                    <span class="li-txt">Contexte</span>
                </a>
            </li>
            <li class="submenu-parent">
                <a href="#">
                    <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                        xml:space="preserve" width="32" height="32">
                        <g>
                            <path d="M504,483.9c-3.4,0-6.7-0.7-9.9-2.2l-452-213c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2L490.4,12.3c6.3-3,13.8-3,20.1,0L958,225.2c8.2,3.9,13.2,11.9,13.2,21c0,
                            9.1-5.1,17.1-13.3,21L514.2,481.6C510.9,483.1,507.5,483.9,504,483.9L504,483.9L504,
                            483.9z M82.1,247.6L504,446.4l414.1-200.1L500.5,47.5L82.1,247.6L82.1,247.6L82.1,247.6z" />
                            <path d="M503.8,990c-3.4,0-6.8-0.7-10-2.2L39.6,774.3c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2l100.2-48l15.6,32.6l-75.6,36.2l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.2,3.9,13.3,11.9,13.3,21c0,9.1-5,17.1-13.2,21L513.9,987.6C510.7,989.2,507.3,
                            990,503.8,990" />
                            <path d="M75,755.3l-22.5-29.3l146.9-70.6l42.2,20.2L75,755.3z" />
                            <path d="M921.6,751.2l-158.4-75.2l41.5-20.7l150.5,71.5L921.6,751.2z" />
                            <path d="M503.7,734.7c-3.2,0-6.5-0.7-9.5-2L39.6,519.1c-8.2-3.9-13.3-11.9-13.4-21c0-9.1,
                            5.1-17.2,13.3-21l100.1-48l15.6,32.6L79.5,498l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.1,3.9,13.2,11.9,13.3,20.9c0,9.1-4.9,17.1-13.1,21L513.9,732.4C510.6,734,507.2,
                            734.7,503.7,734.7" />
                            <path d="M804.3,400.3l150.1,70.9l-32.8,24.7l-158.4-75.2L804.3,400.3z" />
                            <path d="M76.9,499.2l-18.7-31.1l142.1-68.2l41.2,20.5L76.9,499.2z" />
                        </g>
                    </svg><span class="li-txt">Deep learning</span>
                </a>
                <ul class="submenu">
                    <li><a href="nn.html">R√©seaux de neurones</a></li>
                    <li><a href="cnn.html">R√©seaux de neurones convolutionnels</a></li>
                </ul>
            </li>
            <li class="submenu-parent">
                <a href="#">
                    <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                        xml:space="preserve" width="32" height="32">
                        <g>
                            <path d="M761.3,663.3H238.7v65.3h522.7V663.3z M696,
                            271.3H565.3V10H434.7v261.3H304l196,196L696,271.3z M761.3,336.9v65.4l65.3,
                            65h-65.3v65.3h65.3v392H173.3v-588h65.3v-65.3H108V990h784V468L761.3,
                            336.9z M238.7,859.3h522.7V794H238.7V859.3z" />
                        </g>
                    </svg><span class="li-txt">T√©l√©charger</span>
                </a>
                <ul class="submenu">
                    <li>
                        <a href="#">Le contenu du site en pdf</a>
                    </li>
                </ul>
            </li>
        </ul>
        <label id="switch" class="switch">
            <input type="checkbox" onchange="toggleTheme()" id="slider">
            <span class="slider round"></span>
        </label>
        <a href="#" id="github">
            <svg version="1.1" x="0px" y="0px" width="32" height="32" viewBox="0 0 1000 1000" aria-hidden="true"
                enable-background="new 0 0 1000 1000" xml:space="preserve">
                <g>
                    <g transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                        <path fill-rule="evenodd" clip-rule="evenodd"
                            d="M4553.4,
                        4863.8c-619.6-51.6-1218.5-223.7-1769.3-499.1c-805.5-406.2-1404.4-946.6-1903.6-1707.3c-358-543.9-602.4-1160-722.9-1827.8C78.5,399.3,82-447.5,
                        161.1-864c330.5-1724.5,1483.6-3122.1,3077.3-3721c261.6-99.8,364.9-103.3,
                        454.4-13.8c62,62,68.8,113.6,68.8,526.7v454.4h-437.2c-378.6,0-464.7,10.3-602.4,
                        75.7c-271.9,123.9-454.4,323.5-626.5,681.6c-151.5,316.7-344.2,554.2-581.7,
                        722.9c-237.5,168.7-51.6,299.5,285.7,203.1c148-44.8,230.6-96.4,378.6-251.3c103.3-106.7,
                        244.4-268.5,309.8-361.4c75.7-106.7,189.3-206.5,309.8-275.4c165.2-92.9,220.3-106.7,
                        447.5-103.3c141.1,3.5,316.7,27.6,389,51.6c120.5,44.8,137.7,72.3,206.5,275.4c41.3,120.5,110.2,268.5,
                        154.9,323.6l79.2,99.8l-354.6,75.7c-636.8,134.3-998.2,320.1-1325.2,681.6c-337.3,368.3-526.7,943.1-526.7,
                        1604c0,516.3,99.8,846.8,368.3,1211.7l117,161.8l-41.3,210c-72.3,375.2,27.5,1039.6,165.2,1091.2c144.6,55.1,
                        729.7-158.3,1122.1-409.6l161.8-103.3l320.1,62c440.6,89.5,1394.1,92.9,1834.7,3.4l313.3-65.4l302.9,175.6c337.3,
                        192.8,647.1,306.4,874.3,320.1l154.9,10.3l65.4-172.1c86.1-220.3,117.1-709.1,58.5-939.7l-41.3-175.6l154.9-234.1c275.4-419.9,
                        316.7-571.4,316.7-1163.5c-3.4-433.7-13.8-561.1-79.2-791.7c-192.8-671.2-602.4-1108.4-1253-1345.9c-82.6-31-306.4-89.5-502.5-130.8l-354.6-75.7l75.7-92.9c41.3-55.1,
                        106.7-179,144.6-275.4c68.8-161.8,75.7-261.6,86.1-1074c6.9-599,24.1-908.8,51.6-939.7c117.1-144.6,278.8-117.1,836.5,148c671.2,313.2,
                        1363.1,881.2,1820.9,1493.9c426.8,564.5,771.1,1363.1,898.4,2068.8c79.2,437.2,
                        79.2,1215.1,0,1652.3c-199.6,1105-815.8,2182.4-1655.7,2888C7162.6,4560.9,5861.4,4980.8,4553.4,4863.8z" />
                    </g>
                </g>
            </svg>
        </a>
    </nav>
    <!--Main content-->
    <main>
        <aside id="left-aside">
            <nav id="aside-nav">
                <h1>Plan du site</h1>
                <ul id="ul-aside-nav"></ul>
            </nav>
        </aside>
        <article class="main-article">
            <header>
                <h1>Les r√©seaux de neurones</h1>
            </header>
            <section class="main-section">
                <h2>Introduction</h2>
                <section>
                    <h3>Machine learning : limites et cons√©quences</h3>
                    <p>
                        Le machine learning, apprentissage automatique ou apprentissage statistique en fran√ßais, est une
                        technologie d‚Äôintelligence artificielle bas√©e sur le fait que la machine peut apprendre toute
                        seule en se basant
                        sur des mod√®les statistiques permettant d‚Äôeffectuer des analyses pr√©dictives. Cependant, cette
                        m√©thode
                        d‚Äôapprentissage requiert que ces donn√©es qui lui sert d‚Äôexemples soient, au pr√©alable, tri√©es
                        par un expert
                        humain pour ne laisser que les caract√©ristiques n√©cessaires √† l‚Äôapprentissage (extraction de
                        caract√©ristiques).
                        Mais, d√®s que ces caract√©ristiques deviennent trop nombreuses, cette extraction devient
                        quasi-impossible.
                        Et c‚Äôest l√† qu‚Äôintervient les r√©seaux de neurones (r√©seaux de neurones artificiel).
                    </p>
                </section>
                <section>
                    <h3>R√©seau de neurones artificiel</h3>
                    <p>
                        Un r√©seau de neurones artificiels ou Neural Network est un syst√®me informatique s‚Äôinspirant du
                        fonctionnement
                        du cerveau humain pour apprendre. C'est une association, en un graphe plus ou moins complexe,
                        d‚Äôobjets √©l√©mentaires :
                        <q>des neurones formels</q>. Les principaux r√©seaux se distinguent par l‚Äôorganisation du graphe
                        (en couches, complets. . . ).
                    </p>
                    <p>
                        Les r√©seaux de neurones sont largement utilis√©s surtout dans le domaine de la reconnaissance
                        d‚Äôimage, de voix, de d√©tection de cancer, etc. Il existe plusieurs types de r√©seaux de neurones,
                        plut√¥t dire
                        plusieurs architectures de r√©seaux de neurones. Parmi eux, on trouve les r√©seaux de neurones
                        r√©currents(RNN) appel√© aussi les r√©seaux <span class="bold"><q lang="en">Feed-back</q></span>, les r√©seaux de neurones autoorganis√©s, 
                        les r√©seaux de neurones <span class="bold"><q lang="en">Feed forward</q> (propagation avant)</span>, etc. Dans le cadre de ce rapport 
                        nous allons nous concentrer sur les r√©seaux de neurones du type Feed Forward et plus pr√©cis√©ment 
                        sur <a href="#titre16"><span class="bold">les perceptrons multicouches</span></a>.
                    </p>
                    <p>
                        Dans le contexte de la classification d‚Äôimages, balancer des images brutes sur un r√©seau de
                        neurones
                        permettra √† ces r√©seaux de trouver les caract√©ristiques de ces images et enfin pouvoir faire des
                        pr√©dictions
                        sur d‚Äôautres images qu‚Äôil n‚Äôa jamais encore vu. Cependant, les r√©seaux de neurones commun√©ment
                        appel√©
                        <span class="bold">perceptron multicouches (MLP)</span> auront des difficult√©s √† reconnaitre de grandes images. Ceci est
                        d√ª √†
                        cause de la croissance exponentielle du nombre de connexions avec la taille de l'image et du
                        fait que chaque
                        neurone est ¬´ totalement connect√© ¬ª √† chacun des neurones de la couche pr√©c√©dente et suivante.
                        Les MLPs
                        auront aussi de difficult√© √† reconnaitre un objet qui est plac√© dans une autre position, angle
                        diff√©rente par
                        rapport aux images utilis√©s pendant l‚Äôentrainement. Et c‚Äôest ce qui fait intervenir les
                        <span class="bold"><q>r√©seaux de neurones convolutifs</q> (CNN pour Convolutional Neural Network)</span>
                        pour r√©soudre ces genres de probl√®mes.
                    </p>
                </section>
                <section>
                    <h3>Les CNN</h3>
                    <p>
                        Les r√©seaux de neurones convolutifs d√©signent une sous-cat√©gorie de r√©seaux de neurones: ils
                        pr√©sentent en effet toutes les caract√©ristiques des MLPs. Cependant, les CNN sont <span class="bold">sp√©cialement
                        con√ßus pour traiter des images en entr√©e</span>. Et apportent de nouvelles fonctionnalit√©s dans le traitement
                        d‚Äôimages.
                    </p>
                </section>
            </section>

            <section class="main-section">
                <h2>Neurone artificiel (Perceptron)</h2>
                <section>
                    <h3>Le perceptron</h3>
                    <p>
                        Le perceptron aussi commun√©ment appel√© neurone artificiel ou neurone formel, est un algorithme
                        d‚Äôapprentissage automatique supervis√© de <strong class="bold"><a href="contexte.html#titre4">classification</a> binaire</strong>. Il s‚Äôagit du type de r√©seau de
                        neurones artificiels le plus simple. Les perceptrons, ont √©t√© cr√©√©s par 
                        <a href="https://fr.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> en 1958. Son
                        cr√©ateur l‚Äôa compar√© √† des neurones capables de r√©pondre √† des stimuli externes d‚Äôune mani√®re qui imite les
                        vrais neurones biologiques dans sa publication <q class="quotation"><cite>The perceptron : a probabilistic model for
                        information storage and organization in the brain</cite></q>.
                    </p>
                    <p>
                        Le perceptron est un classifieur lin√©aire. Ce type de r√©seau neuronal ne contient aucun cycle
                        (il s'agit d'un
                        r√©seau de neurones √† propagation avant). Dans sa version simplifi√©e, le perceptron est
                        monocouche et n'a
                        qu'une seule sortie (bool√©enne) √† laquelle toutes les entr√©es (bool√©ennes) sont connect√©es. Plus
                        g√©n√©ralement, les entr√©es peuvent √™tre des nombres r√©els.
                    </p>
                    <div class="figure-layout">
                        <figure id="mark1-perceptron">
                            <img src="../img/perceptron.jpeg" alt="Mark1, le premier perceptron du monde"
                                title="Mark1, le premier perceptron du monde">
                            <figcaption>Mark1, le premier perceptron du monde</figcaption>
                        </figure>
                    </div>
                </section>
                <section>
                    <h3>Structure d'un perceptron</h3>
                    <p>
                        Voici comment un neurone est sch√©matiquement repr√©sent√© dans le fonctionnement
                        du cerveau humain.
                    </p>
                    <div class="figure-layout">
                        <figure id="human-neuron">
                            <img src="../img/neuron-human.png" alt="Neurone humain" title="Neurone humain">
                            <figcaption>Neurone humain</figcaption>
                        </figure>
                    </div>
                    <p>
                        Un perceptron est une mod√©lisation math√©matique du fonctionnement d‚Äôun vrai neurone biologique
                        et qui imite grossi√®rement le fonctionnement de ce dernier. Il re√ßoit en param√®tre des entr√©es X
                        dont chaque
                        entr√©e est multipli√©e par un poids w. Toutes ces entr√©es sont alors somm√©es et pass√© dans une
                        fonction dite
                        d‚Äôactivation ou fonction de transfert. Cette fonction va comparer la somme pond√©r√©e √† un √† un
                        certain seuil
                        b pour d√©cider quel sera la sortie y du neurone.
                    </p>

                    <div class="figure-layout">
                        <figure id="perceptron">
                            <img src="">
                            <figcaption>Structure d‚Äôun neurone artificiel avec
                                <span class="bold">n</span>
                                entr√©es <span class="bold">x<sub>i</sub></span>
                                et une sortie y
                            </figcaption>
                        </figure>
                    </div>
                    <div class="list-p">
                        <p>
                            Comme nous pouvons le constater, sa structure est compos√©e de 6 parties :
                        </p>
                        <ul>
                            <li>Les entr√©es ($x_1, x_2, x_3, ..., x_n$)</li>
                            <li>Les poids ($w_1, w_2, w_3, ..., w_n$)</li>
                            <li>Une fonction somme ($\sum$)</li>
                            <li>Un biais ($w_0$)</li>
                            <li>Une fonction d'activation ($h$ qu'on note souvent par la notation grec $\phi$)</li>
                            <li>Une sortie ($y$) </li>
                        </ul>
                    </div>

                    <p>
                        Sur la pr√©c√©dente figure, les entr√©es sont les donn√©es que l‚Äôon fournit au perceptron afin qu‚Äôil
                        calcule un r√©sultat.
                        Ces donn√©es doivent √™tre des mesures en lien avec le cas √©tudi√©. Dans le cadre de ce travail,
                        les mesures
                        sont des pixels. √âtant donn√© que les donn√©es utilis√©es peuvent varier, on peut avoir certains
                        probl√®mes de
                        confiance avec les r√©sultats obtenus
                    </p>
                    <p class="alinea">
                        En effet, si nous imaginons qu‚Äôune entr√©e peut avoir des valeurs qui varient entre 0 et 1 et
                        qu‚Äôune deuxi√®me
                        entr√©e peut avoir des valeurs qui varient entre 0 et 3000, alors la deuxi√®me entr√©e va avoir un
                        impact
                        beaucoup plus important dans le r√©sultat calcul√©. Afin de rem√©dier √† cela, il est imp√©ratif de
                        normaliser les
                        donn√©es en entr√©e. Nous verrons plus tard que la normalisation peut avoir un impact sur
                        l‚Äôapprentissage.
                    </p>
                    <div class="list-p">
                        <p>
                            La fonction de somme pond√©r√©e va faire la somme de la multiplication de chaque entr√©e par
                            son poids
                            correspondant et va ajouter le biais, qui est une valeur constante. Avant de pouvoir
                            utiliser le r√©sultat, la
                            somme est pass√©e dans une fonction d‚Äôactivation.
                        </p>
                        <ul>
                            <li>La fonction <strong class="bold">Sigmo√Øde</strong> : Pour une valeur $\boldsymbol{x}$ quelconque, $\boldsymbol{y}$
                                reste entre 0 et 1. Ce cas est tr√®s utile
                                lorsque nous avons besoin d‚Äôavoir un r√©sultat continu. Elle est g√©n√©ralement not√©e par
                                le symbole
                                grec $\boldsymbol{\sigma}$ repr√©sent√©e par la fonction $\boldsymbol{\sigma(x) = \frac{1}{e^{-x}}}
                                <!--{1 \over exp^{-x}}-->$.
                            </li>
                            <li>
                                La fonction $\boldsymbol{tanh}$ est √©galement appel√©e <span class="bold">tangente
                                hyperbolique</span> : Cette fonction ressemble √† la
                                fonction Sigmo√Øde. La diff√©rence avec la fonction Sigmo√Øde est que la fonction $tanh$
                                produit un r√©sultat compris entre $\boldsymbol{-1}$ et $\boldsymbol{1}$.
                            </li>
                            <li>
                                La fonction <span class="bold" style="font-family:Cambria" >ReLU</span> (Unit√© lin√©aire rectifi√©e) : Cette fonction converge plus rapidement,
                                optimise et
                                produit la valeur souhait√©e plus rapidement. C‚Äôest de loin la fonction d‚Äôactivation la
                                plus populaire
                                utilis√©e dans les <a href="#NT2">couches cach√©es</a> sur un r√©seau de neurones. La fonction $ReLU$ est
                                interpr√©t√©e par
                                la formule : $\boldsymbol{f(x) = max(0,x)}$.
                            </li>
                            <li>
                                La fonction <span class="bold" style="font-family:Cambria" >Softmax</span> : utilis√© dans la <a href="#NT2">couche de sortie</a>, sur un r√©seau de neurones, car
                                il r√©duit les
                                dimensions et peut repr√©senter une distribution cat√©gorique.
                            </li>
                        </ul>
                        <p>(Vous trouverez encore plus d‚Äôautres fonctions sur <a
                                href="https://fr.wikipedia.org/wiki/Fonction_d'activation#Liste_de_fonctions_d'activation_usuelles">Wikip√©dia</a>)
                        </p>
                    </div>
                    <p>
                        La propagation d‚Äôun vecteur d‚Äôentr√©e ($x_i$) √† travers un perceptron s‚Äô√©crit :
                        $$\boldsymbol{p = \sum^{n}_{i=0} x_i w_i - b}$$ (Avec $n$ le nombre d'entr√©es sur le perceptron)
                        $$\boldsymbol{y = \sigma(p)}$$

                    </p>
                    <p>
                        Cela dit, un seul neurone ne suffit pas √† faire des relations plus complexes. Les neurones on
                        peut en
                        associer plein ensemble, les empiler pour cr√©er des couches de neurones pour r√©ussir √† faire des
                        fonctions
                        beaucoup plus compliqu√©es. Et c‚Äôest cet ensemble de couche de neurones li√©es qu‚Äôon appelle un
                        "R√©seau
                        de neurones" (ou plut√¥t dire r√©seau de neurones artificiel).
                    </p>
                </section>
            </section>

            <section class="main-section">
                <h2>Apprentissage</h2>
                <section>
                    <h3>Mode d'apprentissage</h3>
                    <p>
                        L‚Äôapprentissage d‚Äôune MLP peut √™tre faite de plusieurs fa√ßons. On trouve 
                        <strong class="bold" >l‚Äôapprentissage supervis√©</strong>, 
                        <strong class="bold" >non supervis√©</strong>, 
                        <strong class="bold" >par renforcement</strong>, 
                        <strong class="bold" >semi-supervis√©</strong>.
                    </p>
                    <p>
                        Ici sera seulement explicit√© le mode d‚Äôapprentissage <span class="bold" >supervis√©</span>. 
                        Ce qui veut dire que les donn√©es d‚Äôentra√Ænement que vous fournissez √† l‚Äôalgorithme comportent 
                        les solutions d√©sir√©es, appel√©es √©tiquettes (en anglais, labels).
                    </p>
                    <div id="supervised" class="figure-layout">
                        <figure>
                            <img src='../img/supervivee.png' alt='Mode d‚Äôapprentissage' title="Mode d‚Äôapprentissage" />
                            <figcaption>
                                Un jeu d‚Äôentra√Ænement √©tiquet√© pour un apprentissage
                                supervis√© (ex. classification de spam)[A. G√©ron, Hands-On]
                            </figcaption>
                        </figure>
                    </div>
                </section>
                <section id="percep-train">
                    <h3>Apprentissage d'un perceptron</h3>
                    <section>
                        <h4>Contexte</h4>
                        <p>
                            L‚Äôapprentissage consiste √† envoyer des donn√©es au perceptron et √† analyser le r√©sultat.
                            Ensuite, on indique au perceptron quel √©tait le r√©sultat attendu.<br>
                            Par exemple, si on obtient la valeur $0.3$ mais que la valeur attendue √©tait de $1$, alors
                            l‚Äôalgorithme d‚Äôapprentissage
                            du perceptron d√©tecte qu‚Äôil a fait une erreur de $1‚Äì0.3=0.7$. Cette valeur peut √™tre vue
                            comme une fonction de co√ªt et le perceptron doit s‚Äôadapter
                            (s‚Äôajuster) afin de r√©duire le co√ªt au minimum possible.
                        </p>
                        <div class="list-p">
                            <p>Dans un perceptron nous avons deux types de sortie :</p>
                            <ul>
                                <li>Une sortie r√©elle</li>
                                <li>Une sortie attendue</li>
                            </ul>
                            <p>
                                Si la sortie r√©elle est diff√©rente de la sortie attendue on va soit incr√©menter, soit
                                d√©cr√©menter le biais afin
                                d'adapter un vecteur de pond√©ration (la somme pond√©r√©e)
                            </p>
                        </div>
                        <p>
                            Pour cela, nous utilisons un algorithme appel√© r√©tropropagation du gradient qui a pour but
                            de
                            changer les poids de chaque connexion. Un poids ou poids synaptique est un coefficient
                            num√©rique qui est
                            attribu√© √† chacune des entr√©es d‚Äôun neurone, de mani√®re al√©atoire au d√©but puis les poids
                            sont adapt√©s au
                            fur et √† mesure, et qui permet de pond√©rer celles-ci. (Wikip√©dia, 2017).
                        </p>
                        <div class="figure-layout">
                            <figure id="perceptron-monocouche">
                                <img src='../img/perceptron-apprentissage-dark.svg'
                                    alt="Renvoie de l'erreur vers l'arri√®re"
                                    title="Renvoie de l'erreur vers l'arri√®re" />
                                <figcaption>Renvoie de l'erreur vers l'arri√®re</figcaption>
                            </figure>
                        </div>
                        <p>
                            Si toutes les entr√©es sont √† 0, alors peu importe la valeur des poids donn√©s √† chaque
                            connexion, la valeur en sortie sera toujours 0.
                            Dans ce cas, nous pouvons ajouter le biais qui nous permet de modifier la sortie afin que
                            l‚Äôapprentissage se fasse correctement.
                        </p>
                        <p class="alinea">
                            Ainsi, l'apprentissage d'un perceptron consiste √† envoyer des donn√©es et leurs √©tiquettes
                            sur le perceptron. A la fin,
                            le perceptron doit r√©ussir √† trouver les bonnes configurations qui permettent de donn√©es une
                            sortie
                            qui se rapproche au sortie voulue. On aura alors une fonction d'erreur qui va calculer
                            l'erreur faite par le perceptron,
                            et cet erreur sera envoy√©e vers l'arri√®re pour corriger le poids ET/OU le biais pour que la
                            sortie du perceptron
                            se rapproche le plus proche possible du sortie d√©sir√©e la <strong
                                class="italic bold "><q>r√©tropropagation (du gradient)</q></strong>
                            ou en anglais <strong class="italic bold"><q lang="en">Backpropagation</q></strong>. <br>
                            Ce qu'on peut r√©sumer par : <q>Le perceptron √©volue en apprenant de ces erreurs</q> (essaie
                            -
                            erreurs).
                        </p>
                    </section>
                    <section>
                        <h4>Le biais</h4>
                        <p>
                            Pour une bonne compr√©hension de ce qu‚Äôest le biais, nous allons consid√©rer la fonction
                            lin√©aire
                            $y=ax$. Et nous allons essayer de s√©parer deux ensembles de donn√©es (ici bleu et rouge).
                        </p>

                        <div class="figure-layout">
                            <figure id="bias-1">
                                <img src='' alt="S√©paration de deux ensembles" title="S√©paration de deux ensembles" />
                                <figcaption>S√©paration de deux ensembles</figcaption>
                            </figure>
                        </div>

                        <div class="figure-layout">
                            <p>
                                Et comme nous pouvons le constater, la fonction $y=ax$ s√©pare parfaitement les deux
                                ensembles.
                            </p>
                            <figure id="bias-2">
                                <img src='../img/bias_2-dark.svg'
                                    alt="S√©paration de deux ensembles √† l'aide d'une fonction lin√©aire"
                                    title="S√©paration de deux ensembles √† l'aide d'une fonction lin√©aire" />
                                <figcaption>S√©paration de deux ensembles √† l'aide d'une fonction lin√©aire</figcaption>
                            </figure>
                        </div>
                        <div class="figure-layout">
                            <p>
                                Cependant, la limitation de ces fonctions se pr√©sente lorsque les ensembles de donn√©es
                                sont moins faciles
                                √† s√©parer.<br>
                                Sur la figure suivante, on remarque qu‚Äôaucune fonction ne permet de s√©parer les
                                ensembles
                                correctement avec une fonction lin√©aire. C‚Äôest de cette fa√ßon qu‚Äôon introduit le <strong
                                    class="bold">biais</strong>.
                            </p>
                            <figure id="bias-3">
                                <img src="" alt="S√©paration impossible √† l'aide une fonction lin√©aire"
                                    title="S√©paration impossible √† l'aide une fonction lin√©aire" />
                                <figcaption>S√©paration impossible √† l'aide une fonction lin√©aire</figcaption>
                            </figure>
                        </div>
                        <p>
                            Le biais va permettre de rendre la s√©parations d'ensembles facile en permettant de mieux
                            ajuster
                            les limites entres les ensembles. Elle permet, en effet, d'avoir la possibilit√© de d√©placer
                            les droites
                            s√©parateurs sur les axes. Dans ce cas, on a pas forcement une droite qui passe par
                            l'origine. Et c'est effet qui va permettre de trouver
                            les limites entres les ensembles.
                        </p>
                        <div class="figure-layout">
                            <p>
                                Sur la figure suivante, la fonction utilis√©e est $y = ax + b$ (<span class="italic">une
                                    fonction affine</span>)
                                o√π $+b$ correspond au biais. Gr√¢ce √† ce biais, nous avons pu trouver la bonne droite
                                $ùë¶=‚àí0.3+9$ : une fonction affine
                                qui s√©pare bien les ensembles.
                            </p>
                            <figure id="bias-4">
                                <img src="" alt="S√©paration de deux ensemble √† l'aide d'une fonction affine"
                                    title="S√©paration de deux ensemble √† l'aide d'une fonction affine" />
                                <figcaption>S√©paration de deux ensemble √† l'aide d'une fonction affine</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h4>Le probl√®me XOR</h4>
                        <p>
                            Le perceptron comme nous l'avons vu peut prendre plusieurs √©l√©ments en entr√©es et produire
                            un r√©sultat en sortie. Les √©l√©ments d'entr√©es peuvent √™tre de tout type. Jusque-l√†, un
                            un perceptron peut faire plusieurs types de calculs et produire des bons r√©sultats.
                        </p>
                        <p class="alinea">
                            Et pourtant, on peut tomber sur des ensembles dont leur distributions n'est pas facile √†
                            s√©parer
                            en utilisant un seul forme g√©ometrique en l'occurence une droite.
                        </p>
                        <div class="figure-layout">
                            <p>
                                Prenons cet exemple:
                            </p>
                            <p class="alinea">
                                On a vu qu'en utilisant le
                                <strong class="bold">biais</strong>, nous pouvons d√©placer un droite (ou tout autre
                                forme g√©om√©trique)
                                librement sur les axes d'un rep√®re pour trouver la limites entres des ensembles de
                                don√©es.

                            </p>
                            <figure id="xor0">
                                <img src="" alt="Recapitulatif Biais"
                                    title="Deux ensembles s√©par√© par une droite affine">
                                <figcaption>Deux ensembles s√©par√© par une droite affine</figcaption>

                            </figure>
                        </div>
                        <div class="figure-layout">
                            <p>
                                Supposons maintenant que nous avons des ensembles qui se distribuent de la fa√ßon
                                suivante :
                            </p>
                            <figure id="xor1">
                                <img src="" alt="S√©paration impossible avec une seule droite"
                                    title="S√©paration impossible avec une seule droite" />
                                <figcaption>S√©paration impossible avec une seule droite</figcaption>
                            </figure>
                            <p>
                                On peut voir qu'une seule droite ne permet de les s√©parer correctement. Il en faudra
                                deux pour le faire.
                            </p>
                        </div>
                        <div class="figure-layout">
                            <figure id="xor2">
                                <img src="../img/XOR-2-dark.svg" alt="S√©paration avec deux droites"
                                    title="S√©paration avec deux droites">
                                <figcaption>S√©paration d'ensembles avec deux droites</figcaption>
                            </figure>
                        </div>
                        <p>
                            Il faut savoir ici qu'une droite repr√©sente un perceptron et le fait d'avoir utilis√© deux
                            droites
                            montre qu'un seul perceptron ne peut pas r√©soudre des probl√®mes complexes. Il en faudra
                            regrouper plusieurs
                            pour r√©soudre des probl√®mes qui se r√©partissent d'une mani√®re qu'on ne peut pas les d√©couper
                            avec un seul perceptron.
                        </p>
                        <p>
                            Le fait de regrouper les perceptrons pour r√©soudre une t√¢che est ce qui constitue un r√©seaux
                            de neurones.
                        </p>
                        <p>
                            On peut alors comprendre ici qu'un seul neurone ne suffit pas de faire des relations plus
                            complexes.
                            Les neurones on peut en associer plein ensemble, les empiler pour cr√©er des couches de
                            neurones pour
                            r√©ussir √† faire des fonctions beaucoup plus compliqu√©es. Et c‚Äôest cet ensemble de couche de
                            neurones
                            li√©es qu‚Äôon appelle un <q>r√©seau de neurones</q> (ou plut√¥t dire r√©seau de neurones
                            artificiel).
                        </p>
                    </section>
                </section>
            </section>

            <section class="main-section">
                <h2>R√©seaux de neurones</h2>
                <section>
                    <h3>R√©seau de neurones monocouche</h3>
                    <div>
                        <div>
                            <p>Le r√©seau le plus simple est <strong class="bold"> Le perceptron monocouche.</strong></p>
                            <ul>
                                <li>
                                    Il poss√®de $N$ informations en entr√©es.
                                </li>
                                <li>
                                    Il est compos√© de $p$ neurones, que l‚Äôon repr√©sente g√©n√©ralement
                                    align√©s verticalement ou horizontalement.
                                </li>
                                <li>
                                    Chacun des $p$ neurones est connect√© aux $N$ informations d‚Äôentr√©es.
                                </li>
                                <li>La sortie de chacun des $p$ neurones est une est une sortie du r√©seau.</li>
                            </ul>
                        </div>
                        <p>
                            Une utilisation courante est que chaque neurone de la couche repr√©sente une
                            <span class="underline">classe</span>. Pour un exemple $X$ donn√©, on obtient la classe de
                            cet exemple en
                            prenant la plus grande des $p$ sorties
                        </p>
                        <div class="figure-layout">
                            <figure id='monocouche'>
                                <img src="../img/monocouche-dark.svg" alt="R√©seau de neurones monocouche"
                                    title="R√©seau de neurone monocouche">
                                <figcaption>R√©seau de neurones monocouche</figcaption>
                            </figure>
                        </div>
                    </div>
                </section>
                <section>
                    <h3>R√©seau de neurones multicouche</h3>
                    <p>
                        Dans un r√©seau r√©seau de neurones multicouche ou bien MLP pour <span lang="en">Multi Layer
                            Perceptron</span>,
                        les neurones de la premi√®re couche re√ßoivent toutes les informations entr√©es, ceux de la
                        deuxi√®me re√ßoivent toutes les sorties des neurones de la premi√®re couche, et ainsi de suite
                        jusqu‚Äôaux neurones de la dernier couche.
                    </p>
                    <div class="figure-layout">
                        <figure id="NT1">
                            <img src="../img/NT1-dark.svg" alt="R√©seau de neurones multicouche"
                                title="R√©seau de neurones multicouche">
                            <figcaption>R√©seau de neurones multicouche</figcaption>
                        </figure>
                    </div>

                    <p>
                        Pour cr√©er un tel r√©seau, il faut connecter un ensemble de neurone (une couche de neurone) avec
                        un autre ensemble de neurone. Habituellement, chaque neurone d'une couche est connect√© √† tous
                        les
                        neurones de la couche suivante et celle-ci seulement. Ceci nous permet d'introduire la notion de
                        sens de
                        parcours de l'information (de l'activation) au sein d'un r√©seau et donc de d√©finir les concepts
                        de neurones
                        d'entr√©e et neurones de sortie. Par extension, on appelle couche d'entr√©e l'ensemble des
                        neurones d'entr√©e(input layer),
                        couche de sortie l'ensemble des neurones de sortie (output layer). Les couches interm√©diaires
                        n'ayant aucun contact avec l'ext√©rieur sont appel√©s couches cach√©es (hidden layer).
                    </p>
                    <div class="figure-layout">
                        <p>
                            Dans la figure suivante, on voit que chaque neurone d‚Äôune couche est li√© avec tous les
                            neurones de la couche suivante. Un tel r√©seau de neurone est appel√© r√©seau compl√®tement
                            connect√© (fully connected network en Anglais).
                        </p>
                        <figure id="NT2">
                            <img src="" alt="R√©seau de neurones multicouche" title="R√©seau de neurones multicouche">
                            <figcaption>R√©seau de neurones multicouche</figcaption>
                        </figure>
                    </div>
                </section>
                <section>
                    <h3>Fonctionnement d'un r√©seau de neurones</h3>
                    <p>
                        Le principe du fonctionnement d‚Äôun perceptron et d‚Äôun r√©seau de neurones multicouches est
                        essentiellement le m√™me :
                        nous avons toujours des entr√©es et des sorties mais nous avons des couches interm√©diaires
                        appel√©es couches cach√©es.
                        Pour finir, chaque neurone contient aussi une fonction de somme, une fonction d‚Äôactivation et un
                        biais.
                    </p>
                </section>
            </section>

            <section class="main-section">
                <h2>Apprentissage d'un r√©seau de neurones</h2>
                <p>
                    Le principe d'apprentissage d'un r√©sau de neurones est la m√™me que celle que nous avons vu dans la
                    section
                    <a href="#percep-train">
                        <q title="Aller dans la section &quot;Apprentissage d'un perceptron&quot;">
                            Apprentissage d'un perceptron
                        </q>
                    </a>. La diff√©rence est que sur un r√©seau de
                    neurones l'erreur est envoy√© vers tous les neurones de chaque couche et de cette fa√ßon, le r√©seau se
                    r√©ajuste
                    et trouve les bons configurations a adopt√©.

                </p>
                <section>
                    <h3>La R√©tropropagation (du gradient)</h3>
                    <p>
                        La R√©tropropagation (<span class="bold" lang="en">Backpropagation</span> en anglais) est un
                        raccourci pour
                        <q class="quotation">la propagation d'erreurs vers l‚Äôarri√®re </q>, car une erreur est calcul√©e √†
                        la sortie
                        et est distribu√©e vers l‚Äôarri√®re √† travers les couches du
                        r√©seau. On souhaite trouver un minimum global pour une fonction de co√ªt, tout en √©vitant les
                        √©ventuelles vall√©es et minimum
                        locaux qui nous emp√™cherait de converger vers la solution la plus optimis√© pour notre
                        r√©seau de neurones. Cette r√©tropropagation du gradient va se faire via l‚Äôalternation successives
                        entre deux
                        phases : <strong class="bold">phase avant</strong> et <strong class="bold"> phase
                            arri√®re</strong>.
                    </p>
                    <section>
                        <h4>Phase avant</h4>
                        <p class="alinea">
                            C‚Äôest la phase de pr√©diction. On envoi √† notre r√©seau une donn√©e et il va essayer d‚Äôen
                            pr√©dire un
                            r√©sultat (par exemple la classe de sortie). Il va avoir un √©change d‚Äôinformations, de
                            valeurs et de sommes,
                            entre chaque neurone et chaque couche. Les donn√©es transitent de la couche d‚Äôentr√©e vers la
                            couche de
                            sortie.
                        </p>
                        <div class="figure-layout">
                            <figure id="forward-step">
                                <img src="" title="Propagation de l‚Äôinformation vers l‚Äôavant"
                                    alt="Propagation de l‚Äôinformation vers l‚Äôavant" />
                                <figcaption>Phase avant : Propagation de l‚Äôinformation vers l‚Äôavant</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h4>Phase arri√®re</h4>
                        <p class="alinea">
                            C‚Äôest la phase d‚Äôapprentissage. En effet, L'erreur est propag√©e vers l'arri√®re jusqu'√† la
                            couche
                            pr√©c√©dente. √Ä la suite du passage d‚Äôune donn√©e au sein du r√©seau, nous allons avoir un
                            r√©sultat concernant
                            la pr√©diction. Les poids et biais du r√©seau sont initialis√© de fa√ßon al√©atoire, et vont √™tre
                            mis √† jour au fil des
                            entra√Ænements via ce proc√©d√©.
                        </p>
                        <div class="figure-layout">
                            <figure id="feedback-step">
                                <img src="" title="Propagation de l‚Äôinformation vers l‚Äôavant"
                                    alt="Propagation de l‚Äôinformation vers l‚Äôavant" />
                                <figcaption>Phase avant : Propagation de l‚Äôinformation vers l‚Äôavant</figcaption>
                            </figure>
                        </div>
                    </section>
                </section>
                <section>
                    <h3>Fonction de co√ªt</h3>
                    <p>
                        Une fonction de co√ªt commun√©ment appel√©e fonction de perte ou fonction objective (loss function)
                        est une m√©thode qui sert √† √©valuer dans quelle mesure un mod√®le sp√©cifique mod√©lise les donn√©es.
                    </p>
                    <div>
                        <p>On peut citer par exemple : </p>
                        <ul>
                            <li><strong class="bold">Mean Squared Error (MSE)</strong> : Erreur quadratique moyenne
                                g√©n√©ralement utilis√© dans les
                                probl√®me de r√©gression (le dernier couche contiendra un n≈ìud avec une fonction
                                d‚Äôactivation
                                lin√©aire),
                            </li>
                            <li><strong class="bold">Cross-Entropy</strong> : (entropie crois√©e) √©galement appel√©e perte
                                logarithmique utilis√© dans les
                                probl√®mes de classification binaire (Le dernier couche contiendra un neurone avec
                                sigmo√Øde
                                comme fonction d‚Äôactivation),
                            </li>
                            <li><strong class="bold">Cross-Entropy</strong> : pour une classification multi-classe. Le
                                dernier couche contiendra un nombre de
                                neurones correspondant au nombre de classe et chacun aura comme fonction d‚Äôactivation
                                <strong class="bold">Softmax</strong>.
                            </li>
                        </ul>
                    </div>
                </section>
                <section>
                    <h3>La descente du gradient</h3>
                    <p>
                        La Descente de Gradient est un algorithme d‚Äôoptimisation qui permet de trouver le <span
                            class="bold">minimum</span> de
                        n‚Äôimporte quelle fonction <span class="bold">convexe</span> en convergeant progressivement vers
                        ce minimum. Cette descente
                        peut
                        s‚Äôeffectuer soit de mani√®re globale (<span class="bold">batch gradient</span>), soit par des
                        lots (<span class="bold">mini batch gradient</span>),
                        soit de fa√ßon
                        unitaire (<span class="bold">stochastic gradient</span>). La premi√®re consiste √† envoyer au
                        r√©seau la totalit√© des donn√©es
                        d‚Äôun seul
                        trait, et de faire ensuite le calcul du gradient ainsi que la correction des coefficients. Alors
                        que la seconde
                        consiste √† envoyer au r√©seau, les donn√©es par petit groupe d‚Äôune taille d√©finit par
                        l‚Äôutilisateur. La derni√®re
                        quant √† elle, envoi une donn√©e √† la fois dans le r√©seau.
                    </p>
                    <p>
                        Dans notre cas, notre r√©seau qu‚Äôon cr√©era va utiliser la m√©thode par mini batch. En effet,
                        celle-ci permet une
                        meilleure convergence par rapport √† la stochastic, et nous permet de meilleures performances que
                        la batch, car
                        on ne charge pas enti√®rement nos donn√©es.
                    </p>

                    <div class="figure-layout">
                        <figure id="grad">
                            <img src="../img/grad.svg" alt="Descente de gradient" title="Descente de gradient" />
                            <figcaption>Descente de gradient</figcaption>
                        </figure>
                    </div>

                    <section>
                        <h4>Minimisation de la fonction de co√ªt</h4>
                        <p>
                            Dans les r√©seaux de neurones, on utilise l‚Äôalgorithme de la Descente de Gradient dans les
                            probl√®mes
                            d‚Äôapprentissage supervis√© pour minimiser la fonction co√ªt, qui justement est une fonction
                            convexe comme
                            le montre la figure suivante. Les algorithmes d‚Äôoptimisation (√† savoir<strong class="bold">
                                Adam</strong>,
                            <strong class="bold">RAdam</strong>,
                            <strong class="bold">SGD</strong>,
                            <strong class="bold">RMSprop</strong>,
                            etc.) utilisent un <strong class="bold">hyperparam√®tre</strong> nomm√© <strong
                                class="bold italic" lang="en"><q>learning rate</q></strong>
                            (en fran√ßais Taux d‚Äôapprentissage) pour converger vers le minimum de cette fonction.
                        </p>
                        <p>
                            Si le Learning Rate est trop grand, alors vous ferez de trop grands pas dans la descente de
                            gradient. Cela
                            a l‚Äôavantage de descendre rapidement vers le minimum de la fonction co√ªt, mais vous risquez
                            de louper ce
                            minimum en oscillant autour √† l‚Äôinfini‚Ä¶ (Guillaume 2019)
                        </p>
                        <p>
                            Pour √©viter le cas pr√©c√©dent, vous pourriez √™tre tent√© de choisir un Learning Rate tr√®s
                            faible. Mais s‚Äôil est
                            trop faible, alors vous risquez de mettre un temps infini avant de converger vers le minimum
                            de la fonction
                            cout. (Guillaume 2019)
                        </p>
                        <div class="figure-layout">
                            <figure id="min-grad">
                                <img src="" alt="Minimisation de l'erreur par descente de gradient"
                                    title="Minimisation de l'erreur par descente de gradient" />
                                <figcaption>Minimisation de l'erreur par descente de gradient</figcaption>
                            </figure>
                        </div>
                        <p>
                            L'algorithme de descente de gradient utilis√© pour entra√Æner le r√©seau (i.e. mettre √† jour
                            les poids) est donn√© par : ${w'}_i=w_i - {\eta{.}\frac{dE}{dw_i}}$
                        </p>
                        <div>
                            <p>O√π :</p>
                            <ul>
                                <li><span class="bold">$w_i$ : </span>Est la valeur des poids avant mise √† jour</li>
                                <li><span class="bold">${w'}_i$ : </span>Est la valeur des poids apr√®s mise √† jour</li>
                                <li><span class="bold">$\eta$ : </span>Est le taux d'apprentissage (learning rate)</li>
                            </ul>
                        </div>
                    </section>
                </section>
            </section>
            <section class="main-section conclusion">
                <h2 class="center">Conclusion</h2>
                <div>
                    <p>
                        Les principaux √©l√©ments √† retenir sont les suivants :
                    </p>
                    <ul>
                        <li>Un r√©seau de neurones est un syst√®me compos√© de neurones, g√©n√©ralement r√©partis en plusieurs
                            couches connect√©es entre elles.
                        </li>
                        <li>
                            Un tel syst√®me s'utilise pour r√©soudre divers probl√®mes statistiques, mais nous nous
                            int√©ressons ici
                            qu'au probl√®me de classification (tr√®s courant). Dans ce cas, le r√©seau calcule √† partir de
                            l'entr√©e un
                            score (ou probabilit√©) pour chaque classe. La classe attribu√©e √† l'objet en entr√©e
                            correspond √† celle de
                            score le plus √©lev√©
                        </li>
                        <li>
                            Chaque couche re√ßoit en entr√©e des donn√©es et les renvoie transform√©es. Pour cela, elle
                            calcule une
                            combinaison lin√©aire puis applique √©ventuellement une fonction non-lin√©aire, appel√©e
                            fonction
                            d'activation. Les coefficients de la combinaison lin√©aire d√©finissent les param√®tres (ou
                            poids) de la
                            couche
                        </li>
                        <li>
                            Un r√©seau de neurones est construit en empilant les couches : la sortie d'une couche
                            correspond √†
                            l'entr√©e de la suivante.
                        </li>
                        <li>
                            Cet empilement de couches d√©finit la sortie finale du r√©seau comme le r√©sultat d'une
                            fonction
                            diff√©rentiable de l'entr√©e
                        </li>
                        <li>
                            La derni√®re couche calcule les probabilit√©s finales en utilisant pour fonction d'activation
                            la fonction
                            logistique (classification binaire) ou la fonction Softmax (classification multi-classes)
                        </li>
                        <li>
                            Une fonction de perte (loss function) est associ√©e √† la couche finale pour calculer l'erreur
                            de
                            classification. Il s'agit en g√©n√©ral de l'entropie crois√©e (cross entropy).
                        </li>
                        <li>
                            Les valeurs des poids des couches sont apprises par r√©tropropagation du gradient : on
                            calcule
                            progressivement (pour chaque couche, en partant de la fin du r√©seau) les param√®tres qui
                            minimisent
                            la fonction de perte r√©gularis√©e. L'optimisation se fait avec une descente du gradient.
                        </li>
                    </ul>
                </div>
            </section>

            <footer class="hidden">
                <p>Coded by
                    <a href="https://github.com/faouziMohamed/" target="_blank" title="Facebook account">
                        FAOUZI MOHAMED
                    </a>
                </p>
            </footer>
        </article>
    </main>
    <a href="#top" class="to-top"></a>
    <script type="text/javascript" src="../js/switch.js"></script>
    <script type="text/javascript" src="../js/nav.js"></script>
    <script type="text/javascript" src="../js/body.js"></script>
    <noscript>
        <div id="noscript-layout">
            <p id="no-script-before-main">
                <i class="fa fa-warning"></i>
                La page web fonctionne bien avec javascript activ√©
            </p>
        </div>
    </noscript>
</body>


</html>