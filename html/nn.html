<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="utf-8">
        <title>R√©seaux de neurones</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name='author' content="Faouzi Mohamed">   
        <meta name="color-scheme" content="dark light">
        <link rel="stylesheet" type="text/css" href="../css/style.css">
        <script>
            MathJax = {
                tex: {
                    inlineMath: [
                        ['$', '$'],
                        ['\\(', '\\)']
                    ]
                },
                svg: {
                    fontCache: 'global'
                }
            };
        </script>
        <!--<script id="MathJax-script" async src="../mathjax/es5/tex-svg-full.js"></script>-->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
        <noscript>
            <link rel="stylesheet" href="../css/noscript.css"/>
        </noscript>
    </head>
    <body>
        <nav id='header-nav' id="top">
            <div id="menu">
                <svg
                    version="1.1"
                    x="0px" y="0px" 
                    viewBox="0 0 1000 1000" 
                    enable-background="new 0 0 1000 1000" 
                    xml:space="preserve"
                    width="32"
                    height='32'>
                    <g>
                    <path d="M896.6,291.5H103.5c-51.5,0-93.5-41.9-93.5-93.5c0-51.5,
                        41.9-93.5,93.5-93.5h793.1c51.5,0,93.4,41.9,93.4,93.5C990,249.5,
                        948.1,291.5,896.6,291.5z M103.5,130.8c-37.1,0-67.2,30.2-67.2,
                        67.2s30.2,67.2,67.2,67.2h793.1c37,0,67.2-30.2,
                        67.2-67.2s-30.1-67.2-67.2-67.2H103.5z"/>
                        <path d="M733.7,593.5H103.5C51.9,593.5,10,551.5,10,500c0-51.5,
                        41.9-93.4,93.5-93.4h630.2c51.5,0,93.4,41.9,93.4,93.4C827.1,551.5,
                        785.2,593.5,733.7,593.5z M103.5,432.8c-37.1,0-67.2,30.2-67.2,67.2c0,
                        37.1,30.2,67.2,67.2,67.2h630.2c37.1,0,67.2-30.2,
                        67.2-67.2c0-37-30.1-67.2-67.2-67.2H103.5z"/>
                        <path d="M570.9,895.4H103.5c-51.5,0-93.5-41.9-93.5-93.4c0-51.5,41.9-93.5,
                        93.5-93.5h467.4c51.5,0.1,93.4,42,93.4,93.5C664.3,853.6,622.4,895.4,570.9,
                        895.4z M103.5,734.8c-37.1,0-67.2,30.2-67.2,67.2c0,37,30.2,67.1,67.2,67.1h467.4c37,
                        0,67.2-30.1,67.2-67.1s-30.1-67.2-67.2-67.2H103.5z"/>
                    </g>
                </svg>
            </div>
            <ul id="main-list">
                <li>
                    <a href="../index.html">
                        <svg
                        id="home"
                        version="1.1"
                        x="0px" 
                        y="0px" 
                        viewBox="0 0 1000 1000" 
                        enable-background="new 0 0 1000 1000" 
                        xml:space="preserve"
                        width="25"
                        height="25">
                        <g><path d="M834.6,812.2V413.9L488.3,253.7L140.7,422.6v389.6c0,47.8,19.7,86.6,
                            43.9,86.6h43.9V634.5c0-9.5,7.7-17.1,17.1-17.1h151.9c9.5,0,17.1,7.7,17.1,
                            17.1v264.3h333.4C795.9,898.8,834.6,860,834.6,812.2z M624.7,786.1c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V786.1z M624.7,683.4c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V683.4z M727.8,786.1c0,9.5-7.7,17.1-17.1,
                            17.1h-47.2c-9.5,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,
                            17.1,7.7,17.1,17.1V786.1z M727.8,683.4c0,9.5-7.7,17.1-17.1,17.1h-47.2c-9.5,
                            0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,
                            17.1V683.4z"/>
                        <path d="M960.6,317.6l-126-59.6v-44c0-32.8-23-59.3-51.4-59.3c-27,0-49,24.2-51,
                            54.9L513.8,106.1c-14.2-6.7-30.6-6.6-44.6,0.3L38.7,318c-25.5,12.5-36,43.3-23.5,
                            68.7c12.5,25.4,43.3,35.9,68.7,23.5l408.3-200.6l424.4,200.9c7.1,3.4,14.6,4.9,22,
                            4.9c19.2,0,37.7-10.8,46.4-29.4C997.2,360.4,986.2,329.8,960.6,317.6z"/></g>
                        </svg> 
                        <span class="li-label">Acceuil</span>
                    </a>
                </li>
                <li>
                    <a href="contexte.html">
                        <svg 
                        version="1.1" 
                        x="0px" y="0px" 
                        viewBox="0 0 1000 1000" 
                        enable-background="new 0 0 1000 1000" 
                        xml:space="preserve"
                        width="32"
                        height="32">
                        <g><g 
                            transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                            <path d="M290.7,3020.7c-83.7-36.8-169.5-140.9-183.8-222.6c-6.1-36.8-8.2-1270.4-6.1-2744.9c6.1-2669.4,
                                6.1-2679.6,49-2734.7c22.5-30.6,67.4-75.6,98-98l55.1-42.9l3986.7-6.1c2193.5-2.1,
                                4015.3,0,4052,6.1c145,26.6,161.3,53.1,876.2,1484.8C9767.2-236.8,9900,43,9900,
                                108.3c0,65.4-132.8,345.2-682.1,1444c-375.8,751.6-702.6,1384.7-729.1,1409.2c-26.6,
                                24.5-75.6,53.1-106.2,65.4c-40.8,16.3-1268.3,22.5-4043.9,22.5C1070.9,3047.3,341.8,
                                3043.2,290.7,3020.7z M8633.7,
                                1252.1l571.9-1143.7l-571.9-1143.7l-571.9-1143.7H4406.1H750.2V108.3v2287.4h3655.8h3655.8L8633.7,
                                1252.1z"/>
                            <path d="M7042.7,1056C6526,917.1,6205.4,380,6336.1-126.5c136.8-520.8,674-847.6,1180.5-714.8c181.8,
                                47,318.6,126.6,455.4,263.5c251.2,251.2,349.2,590.2,263.5,917C8098.6,868.1,7555.4,1194.9,7042.7,
                                1056z M7414.4,406.5c210.4-87.8,261.4-361.5,98-524.9c-132.8-132.8-320.6-132.8-453.4,0C6805.8,132.9,
                                7085.6,545.4,7414.4,406.5z"/>
                        </g></g>
                        </svg>
                        <span class="li-txt">Contexte</span>
                    </a>
                </li>
                <li class="submenu-parent">
                    <a href="#">
                    <svg 
                    version="1.1" 
                    x="0px" 
                    y="0px" 
                    viewBox="0 0 1000 1000" 
                    enable-background="new 0 0 1000 1000" 
                    xml:space="preserve"
                    width="32"
                    height="32">
                    <g>
                        <path d="M504,483.9c-3.4,0-6.7-0.7-9.9-2.2l-452-213c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2L490.4,12.3c6.3-3,13.8-3,20.1,0L958,225.2c8.2,3.9,13.2,11.9,13.2,21c0,
                            9.1-5.1,17.1-13.3,21L514.2,481.6C510.9,483.1,507.5,483.9,504,483.9L504,483.9L504,
                            483.9z M82.1,247.6L504,446.4l414.1-200.1L500.5,47.5L82.1,247.6L82.1,247.6L82.1,247.6z"/>
                        <path d="M503.8,990c-3.4,0-6.8-0.7-10-2.2L39.6,774.3c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2l100.2-48l15.6,32.6l-75.6,36.2l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.2,3.9,13.3,11.9,13.3,21c0,9.1-5,17.1-13.2,21L513.9,987.6C510.7,989.2,507.3,
                            990,503.8,990"/>
                        <path d="M75,755.3l-22.5-29.3l146.9-70.6l42.2,20.2L75,755.3z"/>
                        <path d="M921.6,751.2l-158.4-75.2l41.5-20.7l150.5,71.5L921.6,751.2z"/>
                        <path d="M503.7,734.7c-3.2,0-6.5-0.7-9.5-2L39.6,519.1c-8.2-3.9-13.3-11.9-13.4-21c0-9.1,
                            5.1-17.2,13.3-21l100.1-48l15.6,32.6L79.5,498l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.1,3.9,13.2,11.9,13.3,20.9c0,9.1-4.9,17.1-13.1,21L513.9,732.4C510.6,734,507.2,
                            734.7,503.7,734.7"/>
                        <path d="M804.3,400.3l150.1,70.9l-32.8,24.7l-158.4-75.2L804.3,400.3z"/>
                        <path d="M76.9,499.2l-18.7-31.1l142.1-68.2l41.2,20.5L76.9,499.2z"/>
                    </g>
                    </svg><span class="li-txt">Deep learning</span>
                    </a>
                    <ul class="submenu">
                        <li><a href="nn.html">R√©seaux de neurones</a></li>
                        <li><a href="cnn.html">R√©seaux de neurones convolutionnels</a></li>
                    </ul>
                </li>
                <li class="submenu-parent">
                    <a href="#">
                        <svg 
                        version="1.1" 
                        x="0px" 
                        y="0px" 
                        viewBox="0 0 1000 1000" 
                        enable-background="new 0 0 1000 1000" 
                        xml:space="preserve"
                        width="32"
                        height="32">
                        <g>
                        <path d="M761.3,663.3H238.7v65.3h522.7V663.3z M696,
                            271.3H565.3V10H434.7v261.3H304l196,196L696,271.3z M761.3,336.9v65.4l65.3,
                            65h-65.3v65.3h65.3v392H173.3v-588h65.3v-65.3H108V990h784V468L761.3,
                            336.9z M238.7,859.3h522.7V794H238.7V859.3z"/>
                        </g>
                        </svg><span class="li-txt">T√©l√©charger</span>
                    </a>
                    <ul class="submenu">
                        <li>
                            <a href="#">Le contenu du site en pdf</a>
                        </li>
                    </ul>
                </li>
            </ul>
            <label id="switch" class="switch">
                <input type="checkbox" onchange="toggleTheme()" id="slider">
                <span class="slider round"></span>
            </label> 
            <a href="#" id="github">
                <svg 
                    version="1.1" 
                    x="0px" 
                    y="0px" 
                    width="32"
                    height="32"
                    viewBox="0 0 1000 1000"
                    aria-hidden="true"
                    enable-background="new 0 0 1000 1000" 
                    xml:space="preserve">
                    <g><g 
                        transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                        <path 
                        fill-rule="evenodd" 
                        clip-rule="evenodd"
                        d="M4553.4,
                        4863.8c-619.6-51.6-1218.5-223.7-1769.3-499.1c-805.5-406.2-1404.4-946.6-1903.6-1707.3c-358-543.9-602.4-1160-722.9-1827.8C78.5,399.3,82-447.5,
                        161.1-864c330.5-1724.5,1483.6-3122.1,3077.3-3721c261.6-99.8,364.9-103.3,
                        454.4-13.8c62,62,68.8,113.6,68.8,526.7v454.4h-437.2c-378.6,0-464.7,10.3-602.4,
                        75.7c-271.9,123.9-454.4,323.5-626.5,681.6c-151.5,316.7-344.2,554.2-581.7,
                        722.9c-237.5,168.7-51.6,299.5,285.7,203.1c148-44.8,230.6-96.4,378.6-251.3c103.3-106.7,
                        244.4-268.5,309.8-361.4c75.7-106.7,189.3-206.5,309.8-275.4c165.2-92.9,220.3-106.7,
                        447.5-103.3c141.1,3.5,316.7,27.6,389,51.6c120.5,44.8,137.7,72.3,206.5,275.4c41.3,120.5,110.2,268.5,
                        154.9,323.6l79.2,99.8l-354.6,75.7c-636.8,134.3-998.2,320.1-1325.2,681.6c-337.3,368.3-526.7,943.1-526.7,
                        1604c0,516.3,99.8,846.8,368.3,1211.7l117,161.8l-41.3,210c-72.3,375.2,27.5,1039.6,165.2,1091.2c144.6,55.1,
                        729.7-158.3,1122.1-409.6l161.8-103.3l320.1,62c440.6,89.5,1394.1,92.9,1834.7,3.4l313.3-65.4l302.9,175.6c337.3,
                        192.8,647.1,306.4,874.3,320.1l154.9,10.3l65.4-172.1c86.1-220.3,117.1-709.1,58.5-939.7l-41.3-175.6l154.9-234.1c275.4-419.9,
                        316.7-571.4,316.7-1163.5c-3.4-433.7-13.8-561.1-79.2-791.7c-192.8-671.2-602.4-1108.4-1253-1345.9c-82.6-31-306.4-89.5-502.5-130.8l-354.6-75.7l75.7-92.9c41.3-55.1,
                        106.7-179,144.6-275.4c68.8-161.8,75.7-261.6,86.1-1074c6.9-599,24.1-908.8,51.6-939.7c117.1-144.6,278.8-117.1,836.5,148c671.2,313.2,
                        1363.1,881.2,1820.9,1493.9c426.8,564.5,771.1,1363.1,898.4,2068.8c79.2,437.2,
                        79.2,1215.1,0,1652.3c-199.6,1105-815.8,2182.4-1655.7,2888C7162.6,4560.9,5861.4,4980.8,4553.4,4863.8z"/>
                    </g></g>
                </svg>               
            </a> 
        </nav>
        <!--Main content-->
        <main>
            <aside id="left-aside">
                <nav id="aside-nav">
                    <h1>Plan du site</h1>
                    <ul id="ul-aside-nav"></ul>
                </nav>
            </aside>
            <article class="main-article">
                <header>
                    <h1>Les r√©seaux de neurones</h1>
                </header>
                <section class="main-section">
                    <h2>Introduction</h2>
                    <section>
                        <h3>Machine learning : limites et cons√©quences</h3>
                        <p>
                            Le machine learning, apprentissage automatique ou apprentissage statistique en fran√ßais, est une
    technologie d‚Äôintelligence artificielle bas√©e sur le fait que la machine peut apprendre toute seule en se basant
    sur des mod√®les statistiques permettant d‚Äôeffectuer des analyses pr√©dictives. Cependant, cette m√©thode
    d‚Äôapprentissage requiert que ces donn√©es qui lui sert d‚Äôexemples soient, au pr√©alable, tri√©es par un expert
    humain pour ne laisser que les caract√©ristiques n√©cessaires √† l‚Äôapprentissage (extraction de caract√©ristiques).
    Mais, d√®s que ces caract√©ristiques deviennent trop nombreuses, cette extraction devient quasi-impossible.
    Et c‚Äôest l√† qu‚Äôintervient les r√©seaux de neurones (r√©seaux de neurones artificiel).
                        </p>
                    </section>
                    <section>
                        <h3>R√©seau de neurones artificiel</h3>
                        <p>
                            Un r√©seau de neurones artificiels ou Neural Network est un syst√®me informatique s‚Äôinspirant du fonctionnement
                             du cerveau humain pour apprendre. C'est une association, en un graphe plus ou moins complexe, d‚Äôobjets √©l√©mentaires : 
                             <q>des neurones formels</q>. Les principaux r√©seaux se distinguent par l‚Äôorganisation du graphe (en couches, complets. . . ).
                        </p>
                        <p>
                            Les r√©seaux de neurones sont largement utilis√©s surtout dans le domaine de la reconnaissance
d‚Äôimage, de voix, de d√©tection de cancer, etc. Il existe plusieurs types de r√©seaux de neurones, plut√¥t dire
plusieurs architectures de r√©seaux de neurones. Parmi eux, on trouve les r√©seaux de neurones r√©current
(RNN) appel√© aussi les r√©seaux ‚ÄúFeed-back‚Äù, les r√©seaux de neurones autoorganis√©s, les r√©seaux de
neurones Feed forward (propagation avant), etc. Dans le cadre de notre rapport nous allons nous concentrer
sur les r√©seaux de neurones du type Feed Forward et plus pr√©cis√©ment sur les perceptrons multicouches.
                        </p>
                        <p>
                            Dans le contexte de la classification d‚Äôimages, balancer des images brutes sur un r√©seau de neurones
permettra √† ces r√©seaux de trouver les caract√©ristiques de ces images et enfin pouvoir faire des pr√©dictions
sur d‚Äôautres images qu‚Äôil n‚Äôa jamais encore vu. Cependant, les r√©seaux de neurones commun√©ment appel√©
perceptron multicouches (MLP) auront des difficult√©s √† reconnaitre de grandes images. Ceci est d√ª √†
cause de la croissance exponentielle du nombre de connexions avec la taille de l'image et du fait que chaque
neurone est ¬´ totalement connect√© ¬ª √† chacun des neurones de la couche pr√©c√©dente et suivante. Les MLPs
auront aussi de difficult√© √† reconnaitre un objet qui est plac√© dans une autre position, angle diff√©rente par
rapport aux images utilis√©s pendant l‚Äôentrainement. Et c‚Äôest ce qui fait intervenir les <q>r√©seaux de neurones
convolutifs</q> (CNN pour Convolutional Neural Network) pour r√©soudre ces genres de probl√®mes.
                        </p>
                    </section>
                    <section>
                        <h3>Les CNN</h3>
                        <p>
                            Les r√©seaux de neurones convolutifs d√©signent une sous-cat√©gorie de r√©seaux de neurones: ils
pr√©sentent en effet toutes les caract√©ristiques des MLPs. Cependant, les CNN sont sp√©cialement con√ßus
pour traiter des images en entr√©e. Et apportent de nouvelles fonctionnalit√©s dans le traitement d‚Äôimages.
                        </p>
                    </section>
                </section>
                
                <section class="main-section">
                    <h2>Les r√©seaux de neurones</h2>
                    <section>
                        <h3>Le perceptron</h3>
                        <p>
                            Le perceptron aussi commun√©ment appel√© neurone artificiel ou neurone formel, est un algorithme
d‚Äôapprentissage automatique supervis√© de classification binaire. Il s‚Äôagit du type de r√©seau de neurones
artificiels le plus simple. Les perceptrons, ont √©t√© cr√©√©s par Frank Rosenblatt en 1958. Son cr√©ateur l‚Äôa
compar√© √† des neurones capables de r√©pondre √† des stimuli externes d‚Äôune mani√®re qui imite les vrais
neurones biologiques dans sa publication ¬´ The perceptron : a probabilistic model for information storage
and organization in the brain¬ª
                        </p>
                        <p>
                            Le perceptron est un classifieur lin√©aire. Ce type de r√©seau neuronal ne contient aucun cycle (il s'agit d'un
r√©seau de neurones √† propagation avant). Dans sa version simplifi√©e, le perceptron est monocouche et n'a
qu'une seule sortie (bool√©enne) √† laquelle toutes les entr√©es (bool√©ennes) sont connect√©es. Plus
g√©n√©ralement, les entr√©es peuvent √™tre des nombres r√©els.
                        </p>
                        <div class="figure-layout">
                            <figure id="mark1-perceptron">
                                <img src="../img/perceptron.jpeg" 
                                     alt="Mark1, le premier perceptron du monde" 
                                     title="Mark1, le premier perceptron du monde">
                                <figcaption>Mark1, le premier perceptron du monde</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h3>Structure d'un perceptron</h3>
                        <p>
                            Voici comment un neurone est sch√©matiquement repr√©sent√© dans le fonctionnement 
                            du cerveau humain.
                        </p>
                        <div class="figure-layout">
                            <figure id="human-neuron">
                                <img src="../img/neuron-human.png"
                                alt="Neurone humain" title="Neurone humain">
                                <figcaption>Neurone humain</figcaption>
                            </figure>
                        </div>
                        <p>
                            Un perceptron est une mod√©lisation math√©matique du fonctionnement d‚Äôun vrai neurone biologique
et qui imite grossi√®rement le fonctionnement de ce dernier. Il re√ßoit en param√®tre des entr√©es X dont chaque
entr√©e est multipli√©e par un poids w. Toutes ces entr√©es sont alors somm√©es et pass√© dans une fonction dite
d‚Äôactivation ou fonction de transfert. Cette fonction va comparer la somme pond√©r√©e √† un √† un certain seuil
b pour d√©cider quel sera la sortie y du neurone.
                        </p>

                        <div class="figure-layout">
                            <figure id="perceptron">
                                <img src="">
                                <figcaption>Structure d‚Äôun neurone artificiel avec n entr√©es x i et une sortie y</figcaption>
                            </figure>
                        </div>
                        <div class="list-p">
                            <p>
                                Comme nous pouvons le constater, sa structure est compos√©e de 6 parties :
                            </p>
                            <ul>
                                <li>Les entr√©es ($x_1, x_2, x_3, ..., x_n$)</li>
                                <li>Les poids ($w_1, w_2, w_3, ..., w_n$)</li>
                                <li>Une fonction somme ($\sum$)</li>
                                <li>Un biais ($w_0$)</li>
                                <li>Une fonction d'activation ($h$ qu'on note souvent par la notation grec $\phi$)</li>
                                <li>Une sortie ($y$)</li>
                            </ul>
                        </div>

                        <p>
                            Sur la pr√©c√©dente, les entr√©es sont les donn√©es que l‚Äôon fournit au perceptron afin qu‚Äôil calcule un r√©sultat.
Ces donn√©es doivent √™tre des mesures en lien avec le cas √©tudi√©. Dans le cadre de ce travail, les mesures
sont des pixels. √âtant donn√© que les donn√©es utilis√©es peuvent varier, on peut avoir certains probl√®mes de
confiance avec les r√©sultats obtenus
                        </p>
                        <p class="alinea">
                            En effet, si nous imaginons qu‚Äôune entr√©e peut avoir des valeurs qui varient entre 0et 1 et qu‚Äôune deuxi√®me
entr√©e peut avoir des valeurs qui varient entre 0 et 3000, alors la deuxi√®me entr√©e va avoir un impact
beaucoup plus important dans le r√©sultat calcul√©. Afin de rem√©dier √† cela, il est imp√©ratif de normaliser les
donn√©es en entr√©e. Nous verrons plus tard que la normalisation peut avoir un impact sur l‚Äôapprentissage.
                        </p>
                        <div class="list-p">
                            <p>
                                La fonction de somme pond√©r√©e va faire la somme de la multiplication de chaque entr√©e par son poids
correspondant et va ajouter le biais, qui est une valeur constante. Avant de pouvoir utiliser le r√©sultat, la
somme est pass√©e dans une fonction d‚Äôactivation.
                            </p>
                            <ul>
                                <li>La fonction <strong class="bold">Sigmo√Øde</strong> : Pour une valeur $x$ quelconque, $y$ reste entre 0 et 1. Ce cas est tr√®s utile
                                    lorsque nous avons besoin d‚Äôavoir un r√©sultat continu. Elle est g√©n√©ralement not√©e par le symbole
                                    grec $\sigma$ repr√©sent√©e par la fonction $\mathbf\sigma(x) = \frac{1}{e^{-x}}<!--{1 \over exp^{-x}}-->$.
                                </li>
                                <li>
                                    La fonction <strong class="bold">$tanh$</strong> est √©galement appel√©e tangente hyperbolique : Cette fonction ressemble √† la
fonction Sigmo√Øde. La diff√©rence avec la fonction Sigmo√Øde est que la fonction $tanh$ produit un
r√©sultat compris entre $-1$ et $1$.
                                </li>
                                <li>
                                    La fonction $ReLU$ (Unit√© lin√©aire rectifi√©e) : Cette fonction converge plus rapidement, optimise et
                                    produit la valeur souhait√©e plus rapidement. C‚Äôest de loin la fonction d‚Äôactivation la plus populaire
                                    utilis√©e dans les couches cach√©es sur un r√©seau de neurones.  La fonction $ReLU$ est interpr√©t√©e par
                                    la formule : $f(x) = max(0,x)$.
                                </li>
                                <li>
                                    La fonction $Softmax$ : utilis√© dans la couche de sortie, sur un r√©seau de neurones, car il r√©duit les
                                    dimensions et peut repr√©senter une distribution cat√©gorique.
                                </li>
                            </ul>
                            <p>(Vous trouverez encore plus d‚Äôautres fonctions sur <a href="https://fr.wikipedia.org/wiki/Fonction_d'activation#Liste_de_fonctions_d'activation_usuelles">Wikip√©dia</a>)</p>
                        </div>
                        <p>
                            La propagation d‚Äôun vecteur d‚Äôentr√©e ($x_i$) √† travers un perceptron s‚Äô√©crit :
                            $$p = \sum^{n}_{i=0} x_i w_i - b$$ (Avec $n$ le nombre d'entr√©es sur le perceptron)
                            $$y = \sigma(p)$$

                        </p>
                        <p>
                            Cela dit, un seul neurone ne suffit pas de faire des relations plus complexes. Les neurones on peut en
associer plein ensemble, les empiler pour cr√©er des couches de neurones pour r√©ussir √† faire des fonctions
beaucoup plus compliqu√©es. Et c‚Äôest cet ensemble de couche de neurones li√©es qu‚Äôon appelle un "R√©seau
de neurones" (ou plut√¥t dire r√©seau de neurones artificiel).
                        </p>
                    </section>
                </section>
                
                <section class="main-section">
                    <h2>Apprentissage</h2>
                    <section>
                        <h3>Mode d'apprentissage</h3>
                        <p>
                            L‚Äôapprentissage d‚Äôune MLP peut √™tre faite de plusieurs fa√ßons. On trouve l‚Äôapprentissage supervis√©,
non supervis√©, par renforcement, semi-supervis√©.
                        </p>
                        <p>
Ici sera seulement explicit√© le mode d‚Äôapprentissage supervis√©. Ce qui veut dire que les donn√©es
d‚Äôentra√Ænement que vous fournissez √† l‚Äôalgorithme comportent les solutions d√©sir√©es, appel√©es √©tiquettes
(en anglais, labels).
                        </p>
                        <div id="supervised">
                            <figure>
                                <img 
                                    src='../img/supervivee.png'
                                    alt='Mode d‚Äôapprentissage'
                                    title="Mode d‚Äôapprentissage"/>
                                <figcaption>
                                    Un jeu d‚Äôentra√Ænement √©tiquet√© pour un apprentissage 
                                    supervis√© (ex. classification de spam)[A. G√©ron, Hands-On]
                                </figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h3>Apprentissage d'un perceptron</h3>
                        <p>
                            L‚Äôapprentissage consiste √† envoyer des donn√©es au perceptron et √† analyser le r√©sultat. 
Ensuite, on indique au perceptron quel √©tait le r√©sultat attendu.<br>
Par exemple, si on obtient la valeur $0.3$ mais que la valeur attendue √©tait de $1$, alors l‚Äôalgorithme d‚Äôapprentissage 
du perceptron d√©tecte qu‚Äôil a fait une erreur de $1‚Äì0.3=0.7$. Cette valeur peut √™tre vue comme une fonction de co√ªt et le perceptron doit s‚Äôadapter
(s‚Äôajuster) afin de r√©duire le co√ªt au minimum possible.
                        </p>
                        <p>
                            Pour cela, nous utilisons un algorithme appel√© r√©tropropagation du gradient qui a pour but de
changer les poids de chaque connexion. Un poids ou poids synaptique est un coefficient num√©rique qui est
attribu√© √† chacune des entr√©es d‚Äôun neurone, de mani√®re al√©atoire au d√©but puis les poids sont adapt√©s au
fur et √† mesure, et qui permet de pond√©rer celles-ci. (Wikip√©dia, 2017).
                        </p>
                        <div>
                            <figure id="perceptron-monocouche">
                                <img 
                                    src='../img/perceptron-apprentissage-dark.svg'
                                    alt="Renvoie de l'erreur vers l'arri√®re"
                                    title="Renvoie de l'erreur vers l'arri√®re" />
                                <figcaption>Renvoie de l'erreur vers l'arri√®re</figcaption>
                            </figure>
                        </div>
                        <p class="alinea">
                            Si toutes les entr√©es sont √† 0, alors peu importe la valeur des poids donn√©s √† chaque connexion, la valeur en sortie sera toujours 0. 
Dans ce cas, nous pouvons ajouter le biais qui nous permet de modifier la sortie afin que l‚Äôapprentissage se fasse correctement.
                        </p>
                        <section>
                            <h4>Le biais</h4>
                            <p>
                                Pour une bonne compr√©hension de ce qu‚Äôest le biais, nous allons consid√©rer la fonction lin√©aire
                                $y=ax$. Et nous allons essayer de s√©parer deux ensembles de donn√©es (ici bleu et rouge).
                            </p>

                            <div>
                                <figure id="bias-1">
                                    <img 
                                        src='' 
                                        alt="S√©paration de deux ensembles"
                                        title="S√©paration de deux ensembles"/>
                                    <figcaption>S√©paration de deux ensembles</figcaption>
                                </figure>
                            </div>

                            <div>
                                <p>
                                    Et comme nous pouvons le constater, la fonction $y=ax$  s√©pare parfaitement les deux ensembles.
                                </p>
                                <figure id="bias-2">
                                    <img 
                                        src='../img/bias_2-dark.svg' 
                                        alt="S√©paration de deux ensembles √† l'aide d'une fonction lin√©aire"
                                        title="S√©paration de deux ensembles √† l'aide d'une fonction lin√©aire"/>
                                    <figcaption>S√©paration de deux ensembles √† l'aide d'une fonction lin√©aire</figcaption>
                                </figure>
                            </div>
                            <div>
                                <p>
                                    Cependant, la limitation de ces fonctions se pr√©sente lorsque les ensembles de donn√©es sont moins faciles
    √† s√©parer.<br>
    Sur la figure suivante, on remarque qu‚Äôaucune fonction ne permet de s√©parer les ensembles
    correctement avec une fonction lin√©aire. C‚Äôest de cette fa√ßon qu‚Äôon introduit le <strong class="bold">biais</strong>.
                                </p>
                                <figure id="bias-3">
                                   <img 
                                        src=""
                                        alt="S√©paration impossible √† l'aide une fonction lin√©aire"
                                        title="S√©paration impossible √† l'aide une fonction lin√©aire"/>
                                   <figcaption>S√©paration impossible √† l'aide une fonction lin√©aire</figcaption> 
                                </figure>
                            </div>
                            <p>
                                Le biais va permettre de rendre la s√©parations d'ensembles facile en permettant de mieux ajuster 
                                les limites entres les ensembles. Elle permet, en effet, d'avoir la possibilit√© de d√©placer les droites
                                s√©parateurs sur les axes. Dans ce cas, on a pas forcement une droite qui passe par l'origine. Et c'est effet qui va permettre de trouver 
                                les limites entres les ensembles.
                            </p>
                            <div> 
                                <p>
                                    Sur la figure suivante, la fonction utilis√©e est $y = ax + b$ (<span class="italic">une fonction affine</span>)
                                    o√π $+b$ correspond au biais. Gr√¢ce √† ce biais, nous avons pu trouver la bonne droite $ùë¶=‚àí0.3+9$ : une fonction affine 
                                     qui s√©pare bien les ensembles.  
                                </p>
                                <figure id="bias-4">
                                    <img 
                                        src=""
                                        alt="S√©paration de deux ensemble √† l'aide d'une fonction affine"
                                        title="S√©paration de deux ensemble √† l'aide d'une fonction affine"/>
                                        <figcaption>S√©paration de deux ensemble √† l'aide d'une fonction affine</figcaption>
                                </figure>
                            </div>
                            
                        </section>
                    </section>
                    <section>
                        <h3>R√©seau de neurones monocouche</h3>
                        <div class="flex-bloc">
                            <div>
                                <p>Le r√©seau le plus simple est <strong class="bold"> Le perceptron monocouche.</strong></p>
                                <ul>
                                    <li>
                                        Il poss√®de $N$ informations en entr√©es.
                                    </li>
                                    <li>
                                        Il est compos√© de $p$ neurones, que l‚Äôon repr√©sente g√©n√©ralement
align√©s verticalement ou horizontalement.
                                    </li>
                                    <li>
                                        Chacun des $p$ neurones est connect√© aux $N$ informations d‚Äôentr√©es.
                                    </li>
                                    <li>La sortie de chacun des $p$ neurones est une est une sortie du r√©seau.</li>
                                </ul>
                            </div>
                            <p>
                                Une utilisation courante est que chaque neurone de la couche repr√©sente une
<span class="underline">classe</span>. Pour un exemple $X$ donn√©, on obtient la classe de cet exemple en
prenant la plus grande des $p$ sorties
                            </p>
                            <div>
                                <figure  id='monocouche'>
                                    <img 
                                        src="../img/monocouche-dark.svg" 
                                        alt="R√©seau de neurones monocouche"
                                        title="R√©seau de neurone monocouche">
                                    <figcaption>R√©seau de neurones monocouche</figcaption>
                                </figure>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>R√©seau de neurones multicouche (MLP pour Multi Layer Perceptron)</h3>
                        <p>
                            Dans ce r√©seau, les neurones de la premi√®re couche re√ßoivent toutes les informations entr√©es, ceux de la
deuxi√®me re√ßoivent toutes les sorties des neurones de la premi√®re couche, et ainsi de suite jusqu‚Äôaux neurones
de sortie.
                        </p>

                        <div class="">

                        </div>
                    </section>
                </section>
                
                <footer>
                    <p>Coded by 
                        <a href="https://github.com/faouziMohamed/" target="_blank" title="Facebook account">
                            FAOUZI MOHAMED
                        </a>
                    </p>
                </footer>
            </article>
        </main>
        <a href="#top" class="to-top"></a>
        <script type="text/javascript" src="../js/switch.js"></script>
        <script type="text/javascript" src="../js/nav.js"></script>
        <script type="text/javascript" src="../js/body.js"></script>
        <noscript >
            <div id="noscript-layout">
                <p id="no-script-before-main">
                    <i class="fa fa-warning"></i>
                    La page web fonctionne bien avec javascript activ√©
                </p>        
            </div>
        </noscript>
    </body>


</html>