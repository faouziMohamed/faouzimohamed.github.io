<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="utf-8">
    <title>Réseaux de neurones</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name='author' content="Faouzi Mohamed">
    <meta name="color-scheme" content="dark light">
    <link rel="stylesheet" type="text/css" href="../css/style.css">
    
 <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {fontCache: 'global'}
        };
    </script>
    <!--<script id="MathJax-script" async src="../mathjax/es5/tex-svg-full.js"></script>-->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <noscript>
        <link rel="stylesheet" href="../css/noscript.css" />
    </noscript>

</head>

<body>
    <nav id='header-nav' id="top">
        <div id="menu-icon-wrapper">
            <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                xml:space="preserve" width="32" height='32'>
                <g>
                    <path d="M896.6,291.5H103.5c-51.5,0-93.5-41.9-93.5-93.5c0-51.5,
                        41.9-93.5,93.5-93.5h793.1c51.5,0,93.4,41.9,93.4,93.5C990,249.5,
                        948.1,291.5,896.6,291.5z M103.5,130.8c-37.1,0-67.2,30.2-67.2,
                        67.2s30.2,67.2,67.2,67.2h793.1c37,0,67.2-30.2,
                        67.2-67.2s-30.1-67.2-67.2-67.2H103.5z" />
                    <path d="M733.7,593.5H103.5C51.9,593.5,10,551.5,10,500c0-51.5,
                        41.9-93.4,93.5-93.4h630.2c51.5,0,93.4,41.9,93.4,93.4C827.1,551.5,
                        785.2,593.5,733.7,593.5z M103.5,432.8c-37.1,0-67.2,30.2-67.2,67.2c0,
                        37.1,30.2,67.2,67.2,67.2h630.2c37.1,0,67.2-30.2,
                        67.2-67.2c0-37-30.1-67.2-67.2-67.2H103.5z" />
                    <path d="M570.9,895.4H103.5c-51.5,0-93.5-41.9-93.5-93.4c0-51.5,41.9-93.5,
                        93.5-93.5h467.4c51.5,0.1,93.4,42,93.4,93.5C664.3,853.6,622.4,895.4,570.9,
                        895.4z M103.5,734.8c-37.1,0-67.2,30.2-67.2,67.2c0,37,30.2,67.1,67.2,67.1h467.4c37,
                        0,67.2-30.1,67.2-67.1s-30.1-67.2-67.2-67.2H103.5z" />
                </g>
            </svg>
        </div>
        <ul id="main-list">
            <li>
                <a href="../index.html">
                    <svg id="home" version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000"
                        enable-background="new 0 0 1000 1000" xml:space="preserve" width="25" height="25">
                        <g>
                            <path d="M834.6,812.2V413.9L488.3,253.7L140.7,422.6v389.6c0,47.8,19.7,86.6,
                            43.9,86.6h43.9V634.5c0-9.5,7.7-17.1,17.1-17.1h151.9c9.5,0,17.1,7.7,17.1,
                            17.1v264.3h333.4C795.9,898.8,834.6,860,834.6,812.2z M624.7,786.1c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V786.1z M624.7,683.4c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V683.4z M727.8,786.1c0,9.5-7.7,17.1-17.1,
                            17.1h-47.2c-9.5,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,
                            17.1,7.7,17.1,17.1V786.1z M727.8,683.4c0,9.5-7.7,17.1-17.1,17.1h-47.2c-9.5,
                            0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,
                            17.1V683.4z" />
                            <path d="M960.6,317.6l-126-59.6v-44c0-32.8-23-59.3-51.4-59.3c-27,0-49,24.2-51,
                            54.9L513.8,106.1c-14.2-6.7-30.6-6.6-44.6,0.3L38.7,318c-25.5,12.5-36,43.3-23.5,
                            68.7c12.5,25.4,43.3,35.9,68.7,23.5l408.3-200.6l424.4,200.9c7.1,3.4,14.6,4.9,22,
                            4.9c19.2,0,37.7-10.8,46.4-29.4C997.2,360.4,986.2,329.8,960.6,317.6z" />
                        </g>
                    </svg>
                    <span class="li-label">Acceuil</span>
                </a>
            </li>
            <li>
                <a href="contexte.html">
                    <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                        xml:space="preserve" width="32" height="32">
                        <g>
                            <g transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                                <path d="M290.7,3020.7c-83.7-36.8-169.5-140.9-183.8-222.6c-6.1-36.8-8.2-1270.4-6.1-2744.9c6.1-2669.4,
                                6.1-2679.6,49-2734.7c22.5-30.6,67.4-75.6,98-98l55.1-42.9l3986.7-6.1c2193.5-2.1,
                                4015.3,0,4052,6.1c145,26.6,161.3,53.1,876.2,1484.8C9767.2-236.8,9900,43,9900,
                                108.3c0,65.4-132.8,345.2-682.1,1444c-375.8,751.6-702.6,1384.7-729.1,1409.2c-26.6,
                                24.5-75.6,53.1-106.2,65.4c-40.8,16.3-1268.3,22.5-4043.9,22.5C1070.9,3047.3,341.8,
                                3043.2,290.7,3020.7z M8633.7,
                                1252.1l571.9-1143.7l-571.9-1143.7l-571.9-1143.7H4406.1H750.2V108.3v2287.4h3655.8h3655.8L8633.7,
                                1252.1z" />
                                <path d="M7042.7,1056C6526,917.1,6205.4,380,6336.1-126.5c136.8-520.8,674-847.6,1180.5-714.8c181.8,
                                47,318.6,126.6,455.4,263.5c251.2,251.2,349.2,590.2,263.5,917C8098.6,868.1,7555.4,1194.9,7042.7,
                                1056z M7414.4,406.5c210.4-87.8,261.4-361.5,98-524.9c-132.8-132.8-320.6-132.8-453.4,0C6805.8,132.9,
                                7085.6,545.4,7414.4,406.5z" />
                            </g>
                        </g>
                    </svg>
                    <span class="li-txt">Contexte</span>
                </a>
            </li>
            <li class="submenu-parent">
                <a href="#">
                    <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                        xml:space="preserve" width="32" height="32">
                        <g>
                            <path d="M504,483.9c-3.4,0-6.7-0.7-9.9-2.2l-452-213c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2L490.4,12.3c6.3-3,13.8-3,20.1,0L958,225.2c8.2,3.9,13.2,11.9,13.2,21c0,
                            9.1-5.1,17.1-13.3,21L514.2,481.6C510.9,483.1,507.5,483.9,504,483.9L504,483.9L504,
                            483.9z M82.1,247.6L504,446.4l414.1-200.1L500.5,47.5L82.1,247.6L82.1,247.6L82.1,247.6z" />
                            <path d="M503.8,990c-3.4,0-6.8-0.7-10-2.2L39.6,774.3c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2l100.2-48l15.6,32.6l-75.6,36.2l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.2,3.9,13.3,11.9,13.3,21c0,9.1-5,17.1-13.2,21L513.9,987.6C510.7,989.2,507.3,
                            990,503.8,990" />
                            <path d="M75,755.3l-22.5-29.3l146.9-70.6l42.2,20.2L75,755.3z" />
                            <path d="M921.6,751.2l-158.4-75.2l41.5-20.7l150.5,71.5L921.6,751.2z" />
                            <path d="M503.7,734.7c-3.2,0-6.5-0.7-9.5-2L39.6,519.1c-8.2-3.9-13.3-11.9-13.4-21c0-9.1,
                            5.1-17.2,13.3-21l100.1-48l15.6,32.6L79.5,498l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.1,3.9,13.2,11.9,13.3,20.9c0,9.1-4.9,17.1-13.1,21L513.9,732.4C510.6,734,507.2,
                            734.7,503.7,734.7" />
                            <path d="M804.3,400.3l150.1,70.9l-32.8,24.7l-158.4-75.2L804.3,400.3z" />
                            <path d="M76.9,499.2l-18.7-31.1l142.1-68.2l41.2,20.5L76.9,499.2z" />
                        </g>
                    </svg><span class="li-txt">Deep learning</span>
                </a>
                <ul class="submenu">
                    <li><a href="nn.html">Réseaux de neurones</a></li>
                    <li><a href="cnn.html">Réseaux de neurones convolutionnels</a></li>
                </ul>
            </li>
            <li class="submenu-parent">
                <a href="#">
                    <svg version="1.1" x="0px" y="0px" viewBox="0 0 1000 1000" enable-background="new 0 0 1000 1000"
                        xml:space="preserve" width="32" height="32">
                        <g>
                            <path d="M761.3,663.3H238.7v65.3h522.7V663.3z M696,
                            271.3H565.3V10H434.7v261.3H304l196,196L696,271.3z M761.3,336.9v65.4l65.3,
                            65h-65.3v65.3h65.3v392H173.3v-588h65.3v-65.3H108V990h784V468L761.3,
                            336.9z M238.7,859.3h522.7V794H238.7V859.3z" />
                        </g>
                    </svg><span class="li-txt">Télécharger</span>
                </a>
                <ul class="submenu">
                    <li>
                        <a href="#">Le contenu du site en pdf</a>
                    </li>
                </ul>
            </li>
        </ul>
        <label id="switch" class="switch">
            <input type="checkbox" onchange="toggleTheme()" id="slider">
            <span class="slider round"></span>
        </label>
        <a href="#" id="github">
            <svg version="1.1" x="0px" y="0px" width="32" height="32" viewBox="0 0 1000 1000" aria-hidden="true"
                enable-background="new 0 0 1000 1000" xml:space="preserve">
                <g>
                    <g transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                        <path fill-rule="evenodd" clip-rule="evenodd"
                            d="M4553.4,
                        4863.8c-619.6-51.6-1218.5-223.7-1769.3-499.1c-805.5-406.2-1404.4-946.6-1903.6-1707.3c-358-543.9-602.4-1160-722.9-1827.8C78.5,399.3,82-447.5,
                        161.1-864c330.5-1724.5,1483.6-3122.1,3077.3-3721c261.6-99.8,364.9-103.3,
                        454.4-13.8c62,62,68.8,113.6,68.8,526.7v454.4h-437.2c-378.6,0-464.7,10.3-602.4,
                        75.7c-271.9,123.9-454.4,323.5-626.5,681.6c-151.5,316.7-344.2,554.2-581.7,
                        722.9c-237.5,168.7-51.6,299.5,285.7,203.1c148-44.8,230.6-96.4,378.6-251.3c103.3-106.7,
                        244.4-268.5,309.8-361.4c75.7-106.7,189.3-206.5,309.8-275.4c165.2-92.9,220.3-106.7,
                        447.5-103.3c141.1,3.5,316.7,27.6,389,51.6c120.5,44.8,137.7,72.3,206.5,275.4c41.3,120.5,110.2,268.5,
                        154.9,323.6l79.2,99.8l-354.6,75.7c-636.8,134.3-998.2,320.1-1325.2,681.6c-337.3,368.3-526.7,943.1-526.7,
                        1604c0,516.3,99.8,846.8,368.3,1211.7l117,161.8l-41.3,210c-72.3,375.2,27.5,1039.6,165.2,1091.2c144.6,55.1,
                        729.7-158.3,1122.1-409.6l161.8-103.3l320.1,62c440.6,89.5,1394.1,92.9,1834.7,3.4l313.3-65.4l302.9,175.6c337.3,
                        192.8,647.1,306.4,874.3,320.1l154.9,10.3l65.4-172.1c86.1-220.3,117.1-709.1,58.5-939.7l-41.3-175.6l154.9-234.1c275.4-419.9,
                        316.7-571.4,316.7-1163.5c-3.4-433.7-13.8-561.1-79.2-791.7c-192.8-671.2-602.4-1108.4-1253-1345.9c-82.6-31-306.4-89.5-502.5-130.8l-354.6-75.7l75.7-92.9c41.3-55.1,
                        106.7-179,144.6-275.4c68.8-161.8,75.7-261.6,86.1-1074c6.9-599,24.1-908.8,51.6-939.7c117.1-144.6,278.8-117.1,836.5,148c671.2,313.2,
                        1363.1,881.2,1820.9,1493.9c426.8,564.5,771.1,1363.1,898.4,2068.8c79.2,437.2,
                        79.2,1215.1,0,1652.3c-199.6,1105-815.8,2182.4-1655.7,2888C7162.6,4560.9,5861.4,4980.8,4553.4,4863.8z" />
                    </g>
                </g>
            </svg>
        </a>
    </nav>
    <!--Main content-->
    <main>
        <aside id="left-aside">
            <nav id="aside-nav">
                <h1>Plan du site</h1>
                <ul id="ul-aside-nav"></ul>
            </nav>
        </aside>
        <article class="main-article">
            <header>
                <h1>Les réseaux de neurones</h1>
            </header>
            <section class="main-section">
                <h2>Introduction</h2>
                <section>
                    <h3>Machine learning : limites et conséquences</h3>
                    <p>
                        Le machine learning, apprentissage automatique ou apprentissage statistique en français, est une
                        technologie d’intelligence artificielle basée sur le fait que la machine peut apprendre toute
                        seule en se basant
                        sur des modèles statistiques permettant d’effectuer des analyses prédictives. Cependant, cette
                        méthode
                        d’apprentissage requiert que ces données qui lui sert d’exemples soient, au préalable, triées
                        par un expert
                        humain pour ne laisser que les caractéristiques nécessaires à l’apprentissage (extraction de
                        caractéristiques).
                        Mais, dès que ces caractéristiques deviennent trop nombreuses, cette extraction devient
                        quasi-impossible.
                        Et c’est là qu’intervient les réseaux de neurones (réseaux de neurones artificiel).
                    </p>
                </section>
                <section>
                    <h3>Réseau de neurones artificiel</h3>
                    <p>
                        Un réseau de neurones artificiels ou Neural Network est un système informatique s’inspirant du
                        fonctionnement
                        du cerveau humain pour apprendre. C'est une association, en un graphe plus ou moins complexe,
                        d’objets élémentaires :
                        <q>des neurones formels</q>. Les principaux réseaux se distinguent par l’organisation du graphe
                        (en couches, complets. . . ).
                    </p>
                    <p>
                        Les réseaux de neurones sont largement utilisés surtout dans le domaine de la reconnaissance
                        d’image, de voix, de détection de cancer, etc. Il existe plusieurs types de réseaux de neurones,
                        plutôt dire
                        plusieurs architectures de réseaux de neurones. Parmi eux, on trouve les réseaux de neurones
                        récurrents(RNN) appelé aussi les réseaux <span class="bold"><q lang="en">Feed-back</q></span>, les réseaux de neurones autoorganisés, 
                        les réseaux de neurones <span class="bold"><q lang="en">Feed forward</q> (propagation avant)</span>, etc. Dans le cadre de ce rapport 
                        nous allons nous concentrer sur les réseaux de neurones du type Feed Forward et plus précisément 
                        sur <a href="#titre16"><span class="bold">les perceptrons multicouches</span></a>.
                    </p>
                    <p>
                        Dans le contexte de la classification d’images, balancer des images brutes sur un réseau de
                        neurones
                        permettra à ces réseaux de trouver les caractéristiques de ces images et enfin pouvoir faire des
                        prédictions
                        sur d’autres images qu’il n’a jamais encore vu. Cependant, les réseaux de neurones communément
                        appelé
                        <span class="bold">perceptron multicouches (MLP)</span> auront des difficultés à reconnaitre de grandes images. Ceci est
                        dû à
                        cause de la croissance exponentielle du nombre de connexions avec la taille de l'image et du
                        fait que chaque
                        neurone est « totalement connecté » à chacun des neurones de la couche précédente et suivante.
                        Les MLPs
                        auront aussi de difficulté à reconnaitre un objet qui est placé dans une autre position, angle
                        différente par
                        rapport aux images utilisés pendant l’entrainement. Et c’est ce qui fait intervenir les
                        <span class="bold"><q>réseaux de neurones convolutifs</q> (CNN pour Convolutional Neural Network)</span>
                        pour résoudre ces genres de problèmes.
                    </p>
                </section>
                <section>
                    <h3>Les CNN</h3>
                    <p>
                        Les réseaux de neurones convolutifs désignent une sous-catégorie de réseaux de neurones: ils
                        présentent en effet toutes les caractéristiques des MLPs. Cependant, les CNN sont <span class="bold">spécialement
                        conçus pour traiter des images en entrée</span>. Et apportent de nouvelles fonctionnalités dans le traitement
                        d’images.
                    </p>
                </section>
            </section>

            <section class="main-section">
                <h2>Neurone artificiel (Perceptron)</h2>
                <section>
                    <h3>Le perceptron</h3>
                    <p>
                        Le perceptron aussi communément appelé neurone artificiel ou neurone formel, est un algorithme
                        d’apprentissage automatique supervisé de <strong class="bold"><a href="contexte.html#titre4">classification</a> binaire</strong>. Il s’agit du type de réseau de
                        neurones artificiels le plus simple. Les perceptrons, ont été créés par 
                        <a href="https://fr.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> en 1958. Son
                        créateur l’a comparé à des neurones capables de répondre à des stimuli externes d’une manière qui imite les
                        vrais neurones biologiques dans sa publication <q class="quotation"><cite>The perceptron : a probabilistic model for
                        information storage and organization in the brain</cite></q>.
                    </p>
                    <p>
                        Le perceptron est un classifieur linéaire. Ce type de réseau neuronal ne contient aucun cycle
                        (il s'agit d'un
                        réseau de neurones à propagation avant). Dans sa version simplifiée, le perceptron est
                        monocouche et n'a
                        qu'une seule sortie (booléenne) à laquelle toutes les entrées (booléennes) sont connectées. Plus
                        généralement, les entrées peuvent être des nombres réels.
                    </p>
                    <div class="figure-layout">
                        <figure id="mark1-perceptron">
                            <img src="../img/perceptron.jpeg" alt="Mark1, le premier perceptron du monde"
                                title="Mark1, le premier perceptron du monde">
                            <figcaption>Mark1, le premier perceptron du monde</figcaption>
                        </figure>
                    </div>
                </section>
                <section>
                    <h3>Structure d'un perceptron</h3>
                    <p>
                        Voici comment un neurone est schématiquement représenté dans le fonctionnement
                        du cerveau humain.
                    </p>
                    <div class="figure-layout">
                        <figure id="human-neuron">
                            <img src="../img/neuron-human.png" alt="Neurone humain" title="Neurone humain">
                            <figcaption>Neurone humain</figcaption>
                        </figure>
                    </div>
                    <p>
                        Un perceptron est une modélisation mathématique du fonctionnement d’un vrai neurone biologique
                        et qui imite grossièrement le fonctionnement de ce dernier. Il reçoit en paramètre des entrées X
                        dont chaque
                        entrée est multipliée par un poids w. Toutes ces entrées sont alors sommées et passé dans une
                        fonction dite
                        d’activation ou fonction de transfert. Cette fonction va comparer la somme pondérée à un à un
                        certain seuil
                        b pour décider quel sera la sortie y du neurone.
                    </p>

                    <div class="figure-layout">
                        <figure id="perceptron">
                            <img src="">
                            <figcaption>Structure d’un neurone artificiel avec
                                <span class="bold">n</span>
                                entrées <span class="bold">x<sub>i</sub></span>
                                et une sortie y
                            </figcaption>
                        </figure>
                    </div>
                    <div class="list-p">
                        <p>
                            Comme nous pouvons le constater, sa structure est composée de 6 parties :
                        </p>
                        <ul>
                            <li>Les entrées ($x_1, x_2, x_3, ..., x_n$)</li>
                            <li>Les poids ($w_1, w_2, w_3, ..., w_n$)</li>
                            <li>Une fonction somme ($\sum$)</li>
                            <li>Un biais ($w_0$)</li>
                            <li>Une fonction d'activation ($h$ qu'on note souvent par la notation grec $\phi$)</li>
                            <li>Une sortie ($y$) </li>
                        </ul>
                    </div>

                    <p>
                        Sur la précédente figure, les entrées sont les données que l’on fournit au perceptron afin qu’il
                        calcule un résultat.
                        Ces données doivent être des mesures en lien avec le cas étudié. Dans le cadre de ce travail,
                        les mesures
                        sont des pixels. Étant donné que les données utilisées peuvent varier, on peut avoir certains
                        problèmes de
                        confiance avec les résultats obtenus
                    </p>
                    <p class="alinea">
                        En effet, si nous imaginons qu’une entrée peut avoir des valeurs qui varient entre 0 et 1 et
                        qu’une deuxième
                        entrée peut avoir des valeurs qui varient entre 0 et 3000, alors la deuxième entrée va avoir un
                        impact
                        beaucoup plus important dans le résultat calculé. Afin de remédier à cela, il est impératif de
                        normaliser les
                        données en entrée. Nous verrons plus tard que la normalisation peut avoir un impact sur
                        l’apprentissage.
                    </p>
                    <div class="list-p">
                        <p>
                            La fonction de somme pondérée va faire la somme de la multiplication de chaque entrée par
                            son poids
                            correspondant et va ajouter le biais, qui est une valeur constante. Avant de pouvoir
                            utiliser le résultat, la
                            somme est passée dans une fonction d’activation.
                        </p>
                        <ul>
                            <li>La fonction <strong class="bold">Sigmoïde</strong> : Pour une valeur $\boldsymbol{x}$ quelconque, $\boldsymbol{y}$
                                reste entre 0 et 1. Ce cas est très utile
                                lorsque nous avons besoin d’avoir un résultat continu. Elle est généralement notée par
                                le symbole
                                grec $\boldsymbol{\sigma}$ représentée par la fonction $\boldsymbol{\sigma(x) = \frac{1}{e^{-x}}}
                                <!--{1 \over exp^{-x}}-->$.
                            </li>
                            <li>
                                La fonction $\boldsymbol{tanh}$ est également appelée <span class="bold">tangente
                                hyperbolique</span> : Cette fonction ressemble à la
                                fonction Sigmoïde. La différence avec la fonction Sigmoïde est que la fonction $tanh$
                                produit un résultat compris entre $\boldsymbol{-1}$ et $\boldsymbol{1}$.
                            </li>
                            <li>
                                La fonction <span class="bold" style="font-family:Cambria" >ReLU</span> (Unité linéaire rectifiée) : Cette fonction converge plus rapidement,
                                optimise et
                                produit la valeur souhaitée plus rapidement. C’est de loin la fonction d’activation la
                                plus populaire
                                utilisée dans les <a href="#NT2">couches cachées</a> sur un réseau de neurones. La fonction $ReLU$ est
                                interprétée par
                                la formule : $\boldsymbol{f(x) = max(0,x)}$.
                            </li>
                            <li>
                                La fonction <span class="bold" style="font-family:Cambria" >Softmax</span> : utilisé dans la <a href="#NT2">couche de sortie</a>, sur un réseau de neurones, car
                                il réduit les
                                dimensions et peut représenter une distribution catégorique.
                            </li>
                        </ul>
                        <p>(Vous trouverez encore plus d’autres fonctions sur <a
                                href="https://fr.wikipedia.org/wiki/Fonction_d'activation#Liste_de_fonctions_d'activation_usuelles">Wikipédia</a>)
                        </p>
                    </div>
                    <p>
                        La propagation d’un vecteur d’entrée ($x_i$) à travers un perceptron s’écrit :
                        $$\boldsymbol{p = \sum^{n}_{i=0} x_i w_i - b}$$ (Avec $n$ le nombre d'entrées sur le perceptron)
                        $$\boldsymbol{y = \sigma(p)}$$

                    </p>
                    <p>
                        Cela dit, un seul neurone ne suffit pas à faire des relations plus complexes. Les neurones on
                        peut en
                        associer plein ensemble, les empiler pour créer des couches de neurones pour réussir à faire des
                        fonctions
                        beaucoup plus compliquées. Et c’est cet ensemble de couche de neurones liées qu’on appelle un
                        "Réseau
                        de neurones" (ou plutôt dire réseau de neurones artificiel).
                    </p>
                </section>
            </section>

            <section class="main-section">
                <h2>Apprentissage</h2>
                <section>
                    <h3>Mode d'apprentissage</h3>
                    <p>
                        L’apprentissage d’une MLP peut être faite de plusieurs façons. On trouve 
                        <strong class="bold" >l’apprentissage supervisé</strong>, 
                        <strong class="bold" >non supervisé</strong>, 
                        <strong class="bold" >par renforcement</strong>, 
                        <strong class="bold" >semi-supervisé</strong>.
                    </p>
                    <p>
                        Ici sera seulement explicité le mode d’apprentissage <span class="bold" >supervisé</span>. 
                        Ce qui veut dire que les données d’entraînement que vous fournissez à l’algorithme comportent 
                        les solutions désirées, appelées étiquettes (en anglais, labels).
                    </p>
                    <div id="supervised" class="figure-layout">
                        <figure>
                            <img src='../img/supervivee.png' alt='Mode d’apprentissage' title="Mode d’apprentissage" />
                            <figcaption>
                                Un jeu d’entraînement étiqueté pour un apprentissage
                                supervisé (ex. classification de spam)[<cite>A. Géron, Hands-On</cite>]
                            </figcaption>
                        </figure>
                    </div>
                </section>
                <section id="percep-train">
                    <h3>Apprentissage d'un perceptron</h3>
                    <section>
                        <h4>Contexte</h4>
                        <p>
                            L’apprentissage consiste à envoyer des données au perceptron et à analyser le résultat.
                            Ensuite, on indique au perceptron quel était le résultat attendu.<br>
                            Par exemple, si on obtient la valeur $0.3$ mais que la valeur attendue était de $1$, alors
                            l’algorithme d’apprentissage
                            du perceptron détecte qu’il a fait une erreur de $1–0.3=0.7$. Cette valeur peut être vue
                            comme une <a href="#titre22">fonction de coût</a> et le perceptron doit <span class="bold" >s’adapter
                            (s’ajuster)</span> afin de réduire le coût au <span class="bold" >minimum</span> possible.
                        </p>
                        <div class="list-p">
                            <p class="p-over-ul">Dans un perceptron nous avons deux types de sortie :</p>
                            <ul>
                                <li>Une sortie réelle</li>
                                <li>Une sortie attendue</li>
                            </ul>
                            <p>
                                Si la sortie réelle est différente de la sortie attendue on va soit incrémenter, soit
                                décrémenter le biais afin
                                d'adapter un vecteur de pondération (la somme pondérée)
                            </p>
                        </div>
                        <p>
                            Pour cela, nous utilisons un algorithme appelé <a href="#titre19"><strong class="normal">rétropropagation</strong> du gradient</a> qui a pour but
                            de
                            changer les <em class="italic">poids de chaque connexion</em>. 
                            Par ailleur, un 
                                <span class="bold inline">poids ou poids synaptique</span> 
                                <span role="definition" class="normal inline">est un coefficient numérique qui est attribué à chacune des entrées d’un neurone, 
                                de manière aléatoire au début puis les poids sont adaptés au fur et à mesure, et qui permet de 
                                pondérer celles-ci.
                                </span> (<cite class="italic">Wikipédia, 2017</cite>).
                        </p>
                        <div class="figure-layout">
                            <figure id="perceptron-monocouche">
                                <img src='../img/perceptron-apprentissage-dark.svg'
                                    alt="Renvoie de l'erreur vers l'arrière"
                                    title="Renvoie de l'erreur vers l'arrière" />
                                <figcaption>Renvoie de l'erreur vers l'arrière</figcaption>
                            </figure>
                        </div>
                        <p>
                            Si toutes les entrées sont à 0, alors peu importe la valeur des poids donnés à chaque
                            connexion, la valeur en sortie sera toujours 0.
                            Dans ce cas, nous pouvons ajouter le <a href="#titre12">biais</a> qui nous permet de modifier la sortie afin que
                            l’apprentissage se fasse correctement.
                        </p>
                        <p class="alinea">
                            Ainsi, l'apprentissage d'un perceptron consiste à envoyer des données et leurs étiquettes
                            sur le perceptron. A la fin,
                            le perceptron doit réussir à trouver les bonnes configurations qui permettent de données une
                            sortie
                            qui se rapproche au sortie voulue. On aura alors une fonction d'erreur qui va calculer
                            l'erreur faite par le perceptron,
                            et cet erreur sera envoyée vers l'arrière pour corriger le poids ET/OU le biais pour que la
                            sortie du perceptron
                            se rapproche le plus proche possible du sortie désirée la <strong
                                class="italic bold "><q>rétropropagation (du gradient)</q></strong>
                            ou en anglais <strong class="italic bold"><q lang="en">Backpropagation</q></strong>. <br>
                            Ce qu'on peut résumer par : <q>Le perceptron évolue en apprenant de ces erreurs</q> (essaie
                            -
                            erreurs).
                        </p>
                    </section>
                    <section>
                        <h4>Le biais</h4>
                        <p>
                            Pour une bonne compréhension de ce qu’est le biais, nous allons considérer la fonction
                            linéaire
                            $\boldsymbol{y=ax}$. Et nous allons essayer de séparer deux ensembles de données 
                            (ici 
                            <span class="bold" >bleu</span> et 
                            <span class="bold" >rouge</span>).
                        </p>

                        <div class="figure-layout">
                            <figure id="bias-1">
                                <img src='' alt="Séparation de deux ensembles" title="Séparation de deux ensembles" />
                                <figcaption>Séparation de deux ensembles</figcaption>
                            </figure>
                        </div>

                        <div class="figure-layout">
                            <p>
                                Et comme nous pouvons le constater, la fonction $y=ax$ sépare parfaitement les deux
                                ensembles.
                            </p>
                            <figure id="bias-2">
                                <img src='../img/bias_2-dark.svg'
                                    alt="Séparation de deux ensembles à l'aide d'une fonction linéaire"
                                    title="Séparation de deux ensembles à l'aide d'une fonction linéaire" />
                                <figcaption>Séparation de deux ensembles à l'aide d'une fonction linéaire</figcaption>
                            </figure>
                        </div>
                        <div class="figure-layout">
                            <p>
                                Cependant, la limitation de ces fonctions se présente lorsque les ensembles de données
                                sont moins faciles
                                à séparer.<br>
                                Sur la figure suivante, on remarque qu’aucune fonction ne permet de séparer les
                                ensembles
                                correctement avec une fonction linéaire. C’est de cette façon qu’on introduit le <strong
                                    class="bold">biais</strong>.
                            </p>
                            <figure id="bias-3">
                                <img src="" alt="Séparation impossible à l'aide une fonction linéaire"
                                    title="Séparation impossible à l'aide une fonction linéaire" />
                                <figcaption>Séparation impossible à l'aide une fonction linéaire</figcaption>
                            </figure>
                        </div>
                        <p>
                            Le biais va permettre de rendre la séparations d'ensembles facile en permettant de mieux
                            ajuster
                            les limites entres les ensembles. Elle permet, en effet, d'avoir la possibilité de déplacer
                            les droites
                            séparateurs sur les axes. Dans ce cas, on a pas forcement une droite qui passe par
                            l'origine. Et c'est effet qui va permettre de trouver
                            les limites entres les ensembles.
                        </p>
                        <div class="figure-layout">
                            <p>
                                Sur la figure suivante, la fonction utilisée est $\boldsymbol{y = ax + b}$ 
                                (<span class="italic">une fonction affine</span>)
                                où $\boldsymbol{+b}$ correspond au biais. Grâce à ce biais, nous avons pu trouver la bonne droite
                                $\boldsymbol{y=−0.3+9}$ : une fonction affine qui sépare bien les ensembles.
                            </p>
                            <figure id="bias-4">
                                <img src="" alt="Séparation de deux ensemble à l'aide d'une fonction affine"
                                    title="Séparation de deux ensemble à l'aide d'une fonction affine" />
                                <figcaption>Séparation de deux ensemble à l'aide d'une fonction affine</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h4>Le problème XOR</h4>
                        <p>
                            Le perceptron comme nous l'avons vu peut prendre plusieurs éléments en entrées et produire
                            un résultat en sortie. Les éléments d'entrées peuvent être de tout type. Jusque-là, un
                            un perceptron accepte plusieurs types d'entrées et produit de bons résultats.
                        </p>
                        <p class="alinea">
                            Et pourtant, on peut tomber sur des ensembles dont leur distributions n'est pas facile à
                            séparer
                            en utilisant un seul forme géometrique en l'occurence une droite.
                        </p>
                        <div class="figure-layout">
                            <p>
                                Prenons cet exemple:
                            </p>
                            <p class="alinea">
                                On a vu qu'en utilisant le
                                <strong class="bold">biais</strong>, nous pouvons déplacer un droite (ou tout autre
                                forme géométrique)
                                librement sur les axes d'un repère pour trouver la limites entres des ensembles de
                                donées.

                            </p>
                            <figure id="xor0">
                                <img src="" alt="Recapitulatif Biais"
                                    title="Deux ensembles séparé par une droite affine">
                                <figcaption>Deux ensembles séparé par une droite affine</figcaption>

                            </figure>
                        </div>
                        <div class="figure-layout">
                            <p>
                                Supposons maintenant que nous avons des ensembles qui se distribuent de la façon
                                suivante :
                            </p>
                            <figure id="xor1">
                                <img src="" alt="Séparation impossible avec une seule droite"
                                    title="Séparation impossible avec une seule droite" />
                                <figcaption>Séparation impossible avec une seule droite</figcaption>
                            </figure>
                            <p>
                                On peut voir qu'une seule droite ne permet de les séparer correctement. Il en faudra
                                deux pour le faire.
                            </p>
                        </div>
                        <div class="figure-layout">
                            <figure id="xor2">
                                <img src="../img/XOR-2-dark.svg" alt="Séparation avec deux droites"
                                    title="Séparation avec deux droites">
                                <figcaption>Séparation d'ensembles avec deux droites</figcaption>
                            </figure>
                        </div>
                        <p>
                            Il faut savoir ici qu'une droite représente un perceptron et le fait d'avoir utilisé deux
                            montre qu'un seul perceptron ne peut pas résoudre des problèmes complexes. Il en faudra
                            regrouper plusieurs pour résoudre des problèmes qui se répartissent d'une manière qu'on 
                            ne peut pas les découper avec un seul.
                        </p>
                        <p>
                            Le fait de regrouper les perceptrons pour résoudre une tâche constitue un réseaux
                            de neurones (
                            <a href="#titre15">Perceptron monocouche</a> ou 
                            <a href="#titre16">perceptron multicouche</a>)
                        </p>
                        <p>
                            On peut alors comprendre ici qu'un seul neurone ne suffit pas de faire des relations plus
                            complexes.
                            Les neurones on peut en associer plein ensemble, les empiler pour créer des couches de
                            neurones pour
                            réussir à faire des fonctions beaucoup plus compliquées. Et c’est cet ensemble de couche de
                            neurones
                            liées qu’on appelle un <q>réseau de neurones</q> (ou plutôt dire réseau de neurones
                            artificiel).
                        </p>
                    </section>
                </section>
            </section>

            <section class="main-section">
                <h2>Réseaux de neurones</h2>
                <section>
                    <h3>Réseau de neurones monocouche</h3>
                    <div>
                        <div>
                            <p>Le réseau le plus simple est <strong class="bold"> Le perceptron monocouche.</strong></p>
                            <ul>
                                <li>
                                    Il possède $N$ informations en entrées.
                                </li>
                                <li>
                                    Il est composé de $p$ neurones, que l’on représente généralement
                                    alignés verticalement ou horizontalement.
                                </li>
                                <li>
                                    Chacun des $p$ neurones est connecté aux $N$ informations d’entrées.
                                </li>
                                <li>La sortie de chacun des $p$ neurones est une est une sortie du réseau.</li>
                            </ul>
                        </div>
                        <p>
                            Une utilisation courante est que chaque neurone de la couche représente une
                            <span class="underline">classe</span>. Pour un exemple $X$ donné, on obtient la classe de
                            cet exemple en
                            prenant la plus grande des $p$ sorties
                        </p>
                        <div class="figure-layout">
                            <figure id='monocouche'>
                                <img src="../img/monocouche-dark.svg" alt="Réseau de neurones monocouche"
                                    title="Réseau de neurone monocouche">
                                <figcaption>Réseau de neurones monocouche</figcaption>
                            </figure>
                        </div>
                    </div>
                </section>
                <section>
                    <h3>Réseau de neurones multicouche</h3>
                    <p>
                        Dans un réseau réseau de neurones multicouche (MLP pour <em lang="en" class="italic">Multi Layer
                            Perceptron</em>),
                        les neurones de la première couche reçoivent toutes les informations entrées, ceux de la
                        deuxième reçoivent toutes les sorties des neurones de la première couche, et ainsi de suite
                        jusqu’aux neurones de la dernière couche.
                    </p>
                    <div class="figure-layout">
                        <figure id="NT1">
                            <img src="../img/NT1-dark.svg" alt="Réseau de neurones multicouche"
                                title="Réseau de neurones multicouche">
                            <figcaption>Réseau de neurones multicouche</figcaption>
                        </figure>
                    </div>

                    <p>
                        Pour créer un tel réseau, il faut connecter un ensemble de neurones (une couche de neurones) avec
                        un autre ensemble de neurones. Habituellement, chaque neurone d'une couche est connecté à tous
                        les
                        neurones de la couche suivante et celle-ci seulement. Ceci nous permet d'introduire la notion de
                        sens de
                        parcours de l'information (de l'activation) au sein d'un réseau et donc de définir les concepts
                        de neurones
                        d'entrée et neurones de sortie. Par extension, on appelle couche d'entrée l'ensemble des
                        neurones d'entrée(<strong class="bold" lang="en">input layer</strong>),
                        couche de sortie l'ensemble des neurones de sortie (<strong class="bold" lang="en">output layer</strong>). 
                        Les couches intermédiaires
                        n'ayant aucun contact avec l'extérieur sont appelés couches cachées (<strong class="bold" lang="en">hidden layer</strong>).
                    </p>
                    <div class="figure-layout">
                        <p>
                            Dans la figure suivante, on voit que chaque neurone d’une couche est lié avec tous les
                            neurones de la couche suivante. Un tel réseau de neurone est appelé réseau complètement
                            connecté (<strong class="bold" lang="en">fully connected network</strong> en Anglais).
                        </p>
                        <figure id="NT2">
                            <img src="" alt="Réseau de neurones multicouche" title="Réseau de neurones multicouche">
                            <figcaption>Réseau de neurones multicouche</figcaption>
                        </figure>
                    </div>
                </section>
                <section>
                    <h3>Fonctionnement d'un réseau de neurones</h3>
                    <p>
                        Le principe de fonctionnement d’un perceptron et d’un réseau de neurones multicouche est
                        essentiellement le même :
                        nous avons toujours des entrées et des sorties mais nous avons des couches intermédiaires
                        appelées couches cachées.
                        Pour finir, chaque neurone contient aussi une fonction de somme, une fonction d’activation et un
                        biais.
                    </p>
                </section>
            </section>

            <section class="main-section">
                <h2>Apprentissage d'un réseau de neurones</h2>
                <p>
                    Le principe d'apprentissage d'un résau de neurones est le même que celle que nous avons vu dans la
                    section
                    <a href="#percep-train">
                        <q title="Aller dans la section &quot;Apprentissage d'un perceptron&quot;">
                            Apprentissage d'un perceptron
                        </q>
                    </a>. La différence est que sur un réseau de
                    neurones l'erreur est envoyé vers tous les neurones de chaque couche et de cette façon, le réseau se
                    réajuste et trouve les bonnes configurations à adopter.
                </p>
                <section>
                    <h3>La Rétropropagation (du gradient)</h3>
                    <p>
                        La Rétropropagation (<span class="bold" lang="en">Backpropagation</span> en anglais) est un
                        raccourci pour
                        <q class="quotation">la propagation d'erreur vers l’arrière </q>, car une erreur est calculée à
                        la sortie et est distribuée vers l’arrière à travers les couches du
                        réseau. On souhaite trouver un minimum global pour une fonction de coût, tout en évitant les
                        éventuelles vallées et minimum
                        locaux qui nous empêcherait de converger vers la solution la plus optimisé pour notre
                        réseau de neurones. Cette rétropropagation du gradient va se faire via l’alternation successives
                        entre deux
                        phases : <strong class="bold">phase avant</strong> et <strong class="bold"> phase
                            arrière</strong>.
                    </p>
                    <section>
                        <h4>Phase avant</h4>
                        <p class="alinea">
                            C’est la phase de prédiction. On envoi à notre réseau une donnée et il va essayer d’en
                            prédire un
                            résultat (par exemple la classe de sortie). Il va avoir un échange d’informations, de
                            valeurs et de sommes,
                            entre chaque neurone et chaque couche. Les données transitent de la couche d’entrée vers la
                            couche de sortie.
                        </p>
                        <div class="figure-layout">
                            <figure id="forward-step">
                                <img src="" title="Propagation de l’information vers l’avant"
                                    alt="Propagation de l’information vers l’avant" />
                                <figcaption>Phase avant : Propagation de l’information vers l’avant</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h4>Phase arrière</h4>
                        <p class="alinea">
                            C’est la phase d’apprentissage. En effet, L'erreur est propagée vers l'arrière jusqu'à la
                            couche
                            précédente. À la suite du passage d’une donnée au sein du réseau, nous allons avoir un
                            résultat concernant
                            la prédiction. Les poids et biais du réseau sont initialisé de façon aléatoire, et vont être
                            mis à jour au fil des
                            entraînements via ce procédé.
                        </p>
                        <div class="figure-layout">
                            <figure id="feedback-step">
                                <img src="" title="Propagation de l’information vers l’avant"
                                    alt="Propagation de l’information vers l’avant" />
                                <figcaption>Phase avant : Propagation de l’information vers l’avant</figcaption>
                            </figure>
                        </div>
                    </section>
                </section>
                <section>
                    <h3>Fonction de coût</h3>
                    <p role="definition">
                        Une fonction de coût communément appelée 
                        <strong class="bold">fonction de perte ou fonction objective 
                            (<span lan="en">loss function</span>)
                        </strong>
                        est une méthode qui sert à évaluer dans quelle mesure un modèle spécifique modélise les données.
                    </p>
                    <div>
                        <p>On peut citer par exemple : </p>
                        <ul>
                            <li><strong class="bold">Mean Squared Error (MSE)</strong> : Erreur quadratique moyenne
                                généralement utilisé dans les
                                problème de régression (le dernier couche contiendra un nœud avec une fonction
                                d’activation
                                linéaire),
                            </li>
                            <li><strong class="bold">Cross-Entropy</strong> : (entropie croisée) également appelée perte
                                logarithmique utilisé dans les
                                problèmes de classification binaire (Le dernier couche contiendra un neurone avec
                                sigmoïde
                                comme fonction d’activation),
                            </li>
                            <li><strong class="bold">Cross-Entropy</strong> : pour une classification multi-classe. Le
                                dernier couche contiendra un nombre de
                                neurones correspondant au nombre de classe et chacun aura comme fonction d’activation
                                <strong class="bold">Softmax</strong>.
                            </li>
                        </ul>
                    </div>
                </section>
                <section>
                    <h3>La descente du gradient</h3>
                    <p>
                        La Descente du Gradient est un algorithme d’optimisation qui permet de trouver le 
                        <span class="bold">minimum</span> de
                        n’importe quelle fonction 
                        <span class="bold">convexe</span> 
                        en convergeant progressivement vers ce minimum. Cette descente peut s’effectuer soit de manière globale 
                        (<span class="bold">batch gradient</span>), soit par des lots 
                        (<span class="bold">mini batch gradient</span>),
                        soit de façonunitaire 
                        (<span class="bold">stochastic gradient</span>). La première consiste à envoyer au
                        réseau la totalité des données d’un seul
                        trait, et de faire ensuite le calcul du gradient ainsi que la correction des coefficients. Alors
                        que la seconde consiste à envoyer au réseau, les données par petit groupe d’une taille définit par
                        l’utilisateur. La dernière quant à elle, envoi une donnée à la fois dans le réseau.
                    </p>
                    <p>
                        Dans notre cas, le réseau qu’on va créer utilisera la méthode par <q lang="en">mini batch</q>. En effet,
                        celle-ci permet une
                        meilleure convergence par rapport à la 
                        <q lang="en">stochastic</q>,
                        et nous permet de meilleures performances que la 
                        <q lang="en">batch gradient</q>, 
                        car on ne charge pas entièrement nos données.
                    </p>

                    <div class="figure-layout">
                        <figure id="grad">
                            <img src="../img/grad.svg" alt="Descente de gradient" title="Descente de gradient" />
                            <figcaption>Descente de gradient</figcaption>
                        </figure>
                    </div>

                    <section>
                        <h4>Minimisation de la fonction de coût</h4>
                        <p>
                            Dans les réseaux de neurones, on utilise l’algorithme de la Descente de Gradient dans les
                            problèmes
                            d’apprentissage supervisé pour minimiser la fonction coût, qui, justement, est une fonction
                            convexe comme
                            le montre la figure suivante. Les algorithmes d’optimisation (à savoir
                            <strong class="bold">Adam</strong>,
                            <strong class="bold">RAdam</strong>,
                            <strong class="bold">SGD</strong>,
                            <strong class="bold">RMSprop</strong>,
                            etc.) utilisent un 
                            <strong class="bold">hyperparamètre</strong> nommé 
                            <strong class="bold italic" lang="en"><q>learning rate</q></strong>
                            (en français Taux d’apprentissage) pour converger vers le minimum de cette fonction.
                        </p>
                        <p>
                            Si le Learning Rate est trop grand, alors vous ferez de trop grands pas dans la descente de
                            gradient. Cela
                            a l’avantage de descendre rapidement vers le minimum de la fonction coût, mais vous risquez
                            de louper ce
                            minimum en oscillant autour à l’infini… (<cite> Guillaume 2019</cite>)
                        </p>
                        <p>
                            Pour éviter le cas précédent, vous pourriez être tenté de choisir un Learning Rate très
                            faible. Mais s’il est
                            trop faible, alors vous risquez de mettre un temps infini avant de converger vers le minimum
                            de la fonction
                            cout. (<cite> Guillaume 2019</cite>)
                        </p>
                        <div class="figure-layout">
                            <figure id="min-grad">
                                <img src="" alt="Minimisation de l'erreur par descente de gradient"
                                    title="Minimisation de l'erreur par descente de gradient" />
                                <figcaption>Minimisation de l'erreur par descente de gradient</figcaption>
                            </figure>
                        </div>
                        <p>
                            L'algorithme de descente de gradient utilisé pour entraîner le réseau (i.e. mettre à jour
                            les poids) est donné par : 
                            <span style="font-size: 140%;">$\boldsymbol{{w'}_i=w_i - {\eta{.}\frac{dE}{dw_i}}}$</span>
                        </p>
                        <div>
                            <p>Où :</p>
                            <ul>
                                <li><span class="bold">$\boldsymbol{w_i}$ : </span>Est la valeur des poids avant mise à jour</li>
                                <li><span class="bold">$\boldsymbol{{w'}_i}$ : </span>Est la valeur des poids après mise à jour</li>
                                <li><span class="bold">$\boldsymbol{\eta}$ : </span>Est le taux d'apprentissage (learning rate)</li>
                            </ul>
                        </div>
                    </section>
                </section>
            </section>
            <section class="main-section conclusion">
                <h2 class="center">Conclusion</h2>
                <div>
                    <p>
                        Les principaux éléments à retenir sont les suivants :
                    </p>
                    <ul>
                        <li>Un réseau de neurones est un système composé de neurones, généralement répartis en plusieurs
                            couches connectées entre elles.
                        </li>
                        <li>
                            Un tel système s'utilise pour résoudre divers problèmes statistiques, mais nous nous
                            intéressons ici
                            qu'au problème de classification (très courant). Dans ce cas, le réseau calcule à partir de
                            l'entrée un
                            score (ou probabilité) pour chaque classe. La classe attribuée à l'objet en entrée
                            correspond à celle de
                            score le plus élevé
                        </li>
                        <li>
                            Chaque couche reçoit en entrée des données et les renvoie transformées. Pour cela, elle
                            calcule une
                            combinaison linéaire puis applique éventuellement une fonction non-linéaire, appelée
                            fonction
                            d'activation. Les coefficients de la combinaison linéaire définissent les paramètres (ou
                            poids) de la
                            couche
                        </li>
                        <li>
                            Un réseau de neurones est construit en empilant les couches : la sortie d'une couche
                            correspond à
                            l'entrée de la suivante.
                        </li>
                        <li>
                            Cet empilement de couches définit la sortie finale du réseau comme le résultat d'une
                            fonction
                            différentiable de l'entrée
                        </li>
                        <li>
                            La dernière couche calcule les probabilités finales en utilisant pour fonction d'activation
                            la fonction
                            logistique (classification binaire) ou la fonction Softmax (classification multi-classes)
                        </li>
                        <li>
                            Une fonction de perte (loss function) est associée à la couche finale pour calculer l'erreur
                            de
                            classification. Il s'agit en général de l'entropie croisée (cross entropy).
                        </li>
                        <li>
                            Les valeurs des poids des couches sont apprises par rétropropagation du gradient : on
                            calcule
                            progressivement (pour chaque couche, en partant de la fin du réseau) les paramètres qui
                            minimisent
                            la fonction de perte régularisée. L'optimisation se fait avec une descente du gradient.
                        </li>
                    </ul>
                </div>
            </section>

            <footer class="hidden">
                <p>Coded by
                    <a href="https://github.com/faouziMohamed/" target="_blank" title="Facebook account">
                        FAOUZI MOHAMED
                    </a>
                </p>
            </footer>
        </article>
    </main>
    <a href="#top" class="to-top"></a>
    <script type="text/javascript" src="../js/switch.js"></script>
    <script type="text/javascript" src="../js/nav.js"></script>
    <script type="text/javascript" src="../js/body.js"></script>
    <noscript>
        <div id="noscript-layout">
            <p id="no-script-before-main">
                <i class="fas fa-exclamation-triangle"></i>
                La page web fonctionne bien avec javascript activé
            </p>
        </div>
    </noscript>
</body>


</html>