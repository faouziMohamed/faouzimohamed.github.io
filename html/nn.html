<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="utf-8">
        <title>Réseaux de neurones</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name='author' content="Faouzi Mohamed">   
        <meta name="color-scheme" content="dark light">
        <link rel="stylesheet" type="text/css" href="../css/style.css">
        <script>
            MathJax = {
                tex: {
                    inlineMath: [
                        ['$', '$'],
                        ['\\(', '\\)']
                    ]
                },
                svg: {
                    fontCache: 'global'
                }
            };
        </script>
        <!--<script id="MathJax-script" async src="../mathjax/es5/tex-svg-full.js"></script>-->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
        <noscript>
            <link rel="stylesheet" href="../css/noscript.css"/>
        </noscript>
    </head>
    <body>
        <nav id='header-nav' id="top">
            <div id="menu">
                <svg
                    version="1.1"
                    x="0px" y="0px" 
                    viewBox="0 0 1000 1000" 
                    enable-background="new 0 0 1000 1000" 
                    xml:space="preserve"
                    width="32"
                    height='32'>
                    <g>
                    <path d="M896.6,291.5H103.5c-51.5,0-93.5-41.9-93.5-93.5c0-51.5,
                        41.9-93.5,93.5-93.5h793.1c51.5,0,93.4,41.9,93.4,93.5C990,249.5,
                        948.1,291.5,896.6,291.5z M103.5,130.8c-37.1,0-67.2,30.2-67.2,
                        67.2s30.2,67.2,67.2,67.2h793.1c37,0,67.2-30.2,
                        67.2-67.2s-30.1-67.2-67.2-67.2H103.5z"/>
                        <path d="M733.7,593.5H103.5C51.9,593.5,10,551.5,10,500c0-51.5,
                        41.9-93.4,93.5-93.4h630.2c51.5,0,93.4,41.9,93.4,93.4C827.1,551.5,
                        785.2,593.5,733.7,593.5z M103.5,432.8c-37.1,0-67.2,30.2-67.2,67.2c0,
                        37.1,30.2,67.2,67.2,67.2h630.2c37.1,0,67.2-30.2,
                        67.2-67.2c0-37-30.1-67.2-67.2-67.2H103.5z"/>
                        <path d="M570.9,895.4H103.5c-51.5,0-93.5-41.9-93.5-93.4c0-51.5,41.9-93.5,
                        93.5-93.5h467.4c51.5,0.1,93.4,42,93.4,93.5C664.3,853.6,622.4,895.4,570.9,
                        895.4z M103.5,734.8c-37.1,0-67.2,30.2-67.2,67.2c0,37,30.2,67.1,67.2,67.1h467.4c37,
                        0,67.2-30.1,67.2-67.1s-30.1-67.2-67.2-67.2H103.5z"/>
                    </g>
                </svg>
            </div>
            <ul id="main-list">
                <li>
                    <a href="../index.html">
                        <svg
                        id="home"
                        version="1.1"
                        x="0px" 
                        y="0px" 
                        viewBox="0 0 1000 1000" 
                        enable-background="new 0 0 1000 1000" 
                        xml:space="preserve"
                        width="25"
                        height="25">
                        <g><path d="M834.6,812.2V413.9L488.3,253.7L140.7,422.6v389.6c0,47.8,19.7,86.6,
                            43.9,86.6h43.9V634.5c0-9.5,7.7-17.1,17.1-17.1h151.9c9.5,0,17.1,7.7,17.1,
                            17.1v264.3h333.4C795.9,898.8,834.6,860,834.6,812.2z M624.7,786.1c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V786.1z M624.7,683.4c0,9.5-7.7,
                            17.1-17.1,17.1h-47.2c-9.4,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,
                            17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,17.1V683.4z M727.8,786.1c0,9.5-7.7,17.1-17.1,
                            17.1h-47.2c-9.5,0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,
                            17.1,7.7,17.1,17.1V786.1z M727.8,683.4c0,9.5-7.7,17.1-17.1,17.1h-47.2c-9.5,
                            0-17.1-7.7-17.1-17.1v-47.2c0-9.4,7.7-17.1,17.1-17.1h47.2c9.4,0,17.1,7.7,17.1,
                            17.1V683.4z"/>
                        <path d="M960.6,317.6l-126-59.6v-44c0-32.8-23-59.3-51.4-59.3c-27,0-49,24.2-51,
                            54.9L513.8,106.1c-14.2-6.7-30.6-6.6-44.6,0.3L38.7,318c-25.5,12.5-36,43.3-23.5,
                            68.7c12.5,25.4,43.3,35.9,68.7,23.5l408.3-200.6l424.4,200.9c7.1,3.4,14.6,4.9,22,
                            4.9c19.2,0,37.7-10.8,46.4-29.4C997.2,360.4,986.2,329.8,960.6,317.6z"/></g>
                        </svg> 
                        <span class="li-label">Acceuil</span>
                    </a>
                </li>
                <li>
                    <a href="contexte.html">
                        <svg 
                        version="1.1" 
                        x="0px" y="0px" 
                        viewBox="0 0 1000 1000" 
                        enable-background="new 0 0 1000 1000" 
                        xml:space="preserve"
                        width="32"
                        height="32">
                        <g><g 
                            transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                            <path d="M290.7,3020.7c-83.7-36.8-169.5-140.9-183.8-222.6c-6.1-36.8-8.2-1270.4-6.1-2744.9c6.1-2669.4,
                                6.1-2679.6,49-2734.7c22.5-30.6,67.4-75.6,98-98l55.1-42.9l3986.7-6.1c2193.5-2.1,
                                4015.3,0,4052,6.1c145,26.6,161.3,53.1,876.2,1484.8C9767.2-236.8,9900,43,9900,
                                108.3c0,65.4-132.8,345.2-682.1,1444c-375.8,751.6-702.6,1384.7-729.1,1409.2c-26.6,
                                24.5-75.6,53.1-106.2,65.4c-40.8,16.3-1268.3,22.5-4043.9,22.5C1070.9,3047.3,341.8,
                                3043.2,290.7,3020.7z M8633.7,
                                1252.1l571.9-1143.7l-571.9-1143.7l-571.9-1143.7H4406.1H750.2V108.3v2287.4h3655.8h3655.8L8633.7,
                                1252.1z"/>
                            <path d="M7042.7,1056C6526,917.1,6205.4,380,6336.1-126.5c136.8-520.8,674-847.6,1180.5-714.8c181.8,
                                47,318.6,126.6,455.4,263.5c251.2,251.2,349.2,590.2,263.5,917C8098.6,868.1,7555.4,1194.9,7042.7,
                                1056z M7414.4,406.5c210.4-87.8,261.4-361.5,98-524.9c-132.8-132.8-320.6-132.8-453.4,0C6805.8,132.9,
                                7085.6,545.4,7414.4,406.5z"/>
                        </g></g>
                        </svg>
                        <span class="li-txt">Contexte</span>
                    </a>
                </li>
                <li class="submenu-parent">
                    <a href="#">
                    <svg 
                    version="1.1" 
                    x="0px" 
                    y="0px" 
                    viewBox="0 0 1000 1000" 
                    enable-background="new 0 0 1000 1000" 
                    xml:space="preserve"
                    width="32"
                    height="32">
                    <g>
                        <path d="M504,483.9c-3.4,0-6.7-0.7-9.9-2.2l-452-213c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2L490.4,12.3c6.3-3,13.8-3,20.1,0L958,225.2c8.2,3.9,13.2,11.9,13.2,21c0,
                            9.1-5.1,17.1-13.3,21L514.2,481.6C510.9,483.1,507.5,483.9,504,483.9L504,483.9L504,
                            483.9z M82.1,247.6L504,446.4l414.1-200.1L500.5,47.5L82.1,247.6L82.1,247.6L82.1,247.6z"/>
                        <path d="M503.8,990c-3.4,0-6.8-0.7-10-2.2L39.6,774.3c-8.1-3.7-13.2-11.7-13.4-20.8c-0.1-9.1,
                            5-17.2,13.1-21.2l100.2-48l15.6,32.6l-75.6,36.2l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.2,3.9,13.3,11.9,13.3,21c0,9.1-5,17.1-13.2,21L513.9,987.6C510.7,989.2,507.3,
                            990,503.8,990"/>
                        <path d="M75,755.3l-22.5-29.3l146.9-70.6l42.2,20.2L75,755.3z"/>
                        <path d="M921.6,751.2l-158.4-75.2l41.5-20.7l150.5,71.5L921.6,751.2z"/>
                        <path d="M503.7,734.7c-3.2,0-6.5-0.7-9.5-2L39.6,519.1c-8.2-3.9-13.3-11.9-13.4-21c0-9.1,
                            5.1-17.2,13.3-21l100.1-48l15.6,32.6L79.5,498l424.1,199.3l417-202l-76-36.1l15.5-32.6l100.3,
                            47.6c8.1,3.9,13.2,11.9,13.3,20.9c0,9.1-4.9,17.1-13.1,21L513.9,732.4C510.6,734,507.2,
                            734.7,503.7,734.7"/>
                        <path d="M804.3,400.3l150.1,70.9l-32.8,24.7l-158.4-75.2L804.3,400.3z"/>
                        <path d="M76.9,499.2l-18.7-31.1l142.1-68.2l41.2,20.5L76.9,499.2z"/>
                    </g>
                    </svg><span class="li-txt">Deep learning</span>
                    </a>
                    <ul class="submenu">
                        <li><a href="nn.html">Réseaux de neurones</a></li>
                        <li><a href="cnn.html">Réseaux de neurones convolutionnels</a></li>
                    </ul>
                </li>
                <li class="submenu-parent">
                    <a href="#">
                        <svg 
                        version="1.1" 
                        x="0px" 
                        y="0px" 
                        viewBox="0 0 1000 1000" 
                        enable-background="new 0 0 1000 1000" 
                        xml:space="preserve"
                        width="32"
                        height="32">
                        <g>
                        <path d="M761.3,663.3H238.7v65.3h522.7V663.3z M696,
                            271.3H565.3V10H434.7v261.3H304l196,196L696,271.3z M761.3,336.9v65.4l65.3,
                            65h-65.3v65.3h65.3v392H173.3v-588h65.3v-65.3H108V990h784V468L761.3,
                            336.9z M238.7,859.3h522.7V794H238.7V859.3z"/>
                        </g>
                        </svg><span class="li-txt">Télécharger</span>
                    </a>
                    <ul class="submenu">
                        <li>
                            <a href="#">Le contenu du site en pdf</a>
                        </li>
                    </ul>
                </li>
            </ul>
            <label id="switch" class="switch">
                <input type="checkbox" onchange="toggleTheme()" id="slider">
                <span class="slider round"></span>
            </label> 
            <a href="#" id="github">
                <svg 
                    version="1.1" 
                    x="0px" 
                    y="0px" 
                    width="32"
                    height="32"
                    viewBox="0 0 1000 1000"
                    aria-hidden="true"
                    enable-background="new 0 0 1000 1000" 
                    xml:space="preserve">
                    <g><g 
                        transform="translate(0.000000,511.000000) scale(0.100000,-0.100000)">
                        <path 
                        fill-rule="evenodd" 
                        clip-rule="evenodd"
                        d="M4553.4,
                        4863.8c-619.6-51.6-1218.5-223.7-1769.3-499.1c-805.5-406.2-1404.4-946.6-1903.6-1707.3c-358-543.9-602.4-1160-722.9-1827.8C78.5,399.3,82-447.5,
                        161.1-864c330.5-1724.5,1483.6-3122.1,3077.3-3721c261.6-99.8,364.9-103.3,
                        454.4-13.8c62,62,68.8,113.6,68.8,526.7v454.4h-437.2c-378.6,0-464.7,10.3-602.4,
                        75.7c-271.9,123.9-454.4,323.5-626.5,681.6c-151.5,316.7-344.2,554.2-581.7,
                        722.9c-237.5,168.7-51.6,299.5,285.7,203.1c148-44.8,230.6-96.4,378.6-251.3c103.3-106.7,
                        244.4-268.5,309.8-361.4c75.7-106.7,189.3-206.5,309.8-275.4c165.2-92.9,220.3-106.7,
                        447.5-103.3c141.1,3.5,316.7,27.6,389,51.6c120.5,44.8,137.7,72.3,206.5,275.4c41.3,120.5,110.2,268.5,
                        154.9,323.6l79.2,99.8l-354.6,75.7c-636.8,134.3-998.2,320.1-1325.2,681.6c-337.3,368.3-526.7,943.1-526.7,
                        1604c0,516.3,99.8,846.8,368.3,1211.7l117,161.8l-41.3,210c-72.3,375.2,27.5,1039.6,165.2,1091.2c144.6,55.1,
                        729.7-158.3,1122.1-409.6l161.8-103.3l320.1,62c440.6,89.5,1394.1,92.9,1834.7,3.4l313.3-65.4l302.9,175.6c337.3,
                        192.8,647.1,306.4,874.3,320.1l154.9,10.3l65.4-172.1c86.1-220.3,117.1-709.1,58.5-939.7l-41.3-175.6l154.9-234.1c275.4-419.9,
                        316.7-571.4,316.7-1163.5c-3.4-433.7-13.8-561.1-79.2-791.7c-192.8-671.2-602.4-1108.4-1253-1345.9c-82.6-31-306.4-89.5-502.5-130.8l-354.6-75.7l75.7-92.9c41.3-55.1,
                        106.7-179,144.6-275.4c68.8-161.8,75.7-261.6,86.1-1074c6.9-599,24.1-908.8,51.6-939.7c117.1-144.6,278.8-117.1,836.5,148c671.2,313.2,
                        1363.1,881.2,1820.9,1493.9c426.8,564.5,771.1,1363.1,898.4,2068.8c79.2,437.2,
                        79.2,1215.1,0,1652.3c-199.6,1105-815.8,2182.4-1655.7,2888C7162.6,4560.9,5861.4,4980.8,4553.4,4863.8z"/>
                    </g></g>
                </svg>               
            </a> 
        </nav>
        <!--Main content-->
        <main>
            <aside id="left-aside">
                <nav id="aside-nav">
                    <h1>Plan du site</h1>
                    <ul id="ul-aside-nav"></ul>
                </nav>
            </aside>
            <article class="main-article">
                <header>
                    <h1>Les réseaux de neurones</h1>
                </header>
                <section class="main-section">
                    <h2>Introduction</h2>
                    <section>
                        <h3>Machine learning : limites et conséquences</h3>
                        <p>
                            Le machine learning, apprentissage automatique ou apprentissage statistique en français, est une
    technologie d’intelligence artificielle basée sur le fait que la machine peut apprendre toute seule en se basant
    sur des modèles statistiques permettant d’effectuer des analyses prédictives. Cependant, cette méthode
    d’apprentissage requiert que ces données qui lui sert d’exemples soient, au préalable, triées par un expert
    humain pour ne laisser que les caractéristiques nécessaires à l’apprentissage (extraction de caractéristiques).
    Mais, dès que ces caractéristiques deviennent trop nombreuses, cette extraction devient quasi-impossible.
    Et c’est là qu’intervient les réseaux de neurones (réseaux de neurones artificiel).
                        </p>
                    </section>
                    <section>
                        <h3>Réseau de neurones artificiel</h3>
                        <p>
                            Un réseau de neurones artificiels ou Neural Network est un système informatique s’inspirant du fonctionnement
                             du cerveau humain pour apprendre. C'est une association, en un graphe plus ou moins complexe, d’objets élémentaires : 
                             <q>des neurones formels</q>. Les principaux réseaux se distinguent par l’organisation du graphe (en couches, complets. . . ).
                        </p>
                        <p>
                            Les réseaux de neurones sont largement utilisés surtout dans le domaine de la reconnaissance
d’image, de voix, de détection de cancer, etc. Il existe plusieurs types de réseaux de neurones, plutôt dire
plusieurs architectures de réseaux de neurones. Parmi eux, on trouve les réseaux de neurones récurrent
(RNN) appelé aussi les réseaux “Feed-back”, les réseaux de neurones autoorganisés, les réseaux de
neurones Feed forward (propagation avant), etc. Dans le cadre de notre rapport nous allons nous concentrer
sur les réseaux de neurones du type Feed Forward et plus précisément sur les perceptrons multicouches.
                        </p>
                        <p>
                            Dans le contexte de la classification d’images, balancer des images brutes sur un réseau de neurones
permettra à ces réseaux de trouver les caractéristiques de ces images et enfin pouvoir faire des prédictions
sur d’autres images qu’il n’a jamais encore vu. Cependant, les réseaux de neurones communément appelé
perceptron multicouches (MLP) auront des difficultés à reconnaitre de grandes images. Ceci est dû à
cause de la croissance exponentielle du nombre de connexions avec la taille de l'image et du fait que chaque
neurone est « totalement connecté » à chacun des neurones de la couche précédente et suivante. Les MLPs
auront aussi de difficulté à reconnaitre un objet qui est placé dans une autre position, angle différente par
rapport aux images utilisés pendant l’entrainement. Et c’est ce qui fait intervenir les <q>réseaux de neurones
convolutifs</q> (CNN pour Convolutional Neural Network) pour résoudre ces genres de problèmes.
                        </p>
                    </section>
                    <section>
                        <h3>Les CNN</h3>
                        <p>
                            Les réseaux de neurones convolutifs désignent une sous-catégorie de réseaux de neurones: ils
présentent en effet toutes les caractéristiques des MLPs. Cependant, les CNN sont spécialement conçus
pour traiter des images en entrée. Et apportent de nouvelles fonctionnalités dans le traitement d’images.
                        </p>
                    </section>
                </section>
                
                <section class="main-section">
                    <h2>Les réseaux de neurones</h2>
                    <section>
                        <h3>Le perceptron</h3>
                        <p>
                            Le perceptron aussi communément appelé neurone artificiel ou neurone formel, est un algorithme
d’apprentissage automatique supervisé de classification binaire. Il s’agit du type de réseau de neurones
artificiels le plus simple. Les perceptrons, ont été créés par Frank Rosenblatt en 1958. Son créateur l’a
comparé à des neurones capables de répondre à des stimuli externes d’une manière qui imite les vrais
neurones biologiques dans sa publication « The perceptron : a probabilistic model for information storage
and organization in the brain»
                        </p>
                        <p>
                            Le perceptron est un classifieur linéaire. Ce type de réseau neuronal ne contient aucun cycle (il s'agit d'un
réseau de neurones à propagation avant). Dans sa version simplifiée, le perceptron est monocouche et n'a
qu'une seule sortie (booléenne) à laquelle toutes les entrées (booléennes) sont connectées. Plus
généralement, les entrées peuvent être des nombres réels.
                        </p>
                        <div class="figure-layout">
                            <figure id="mark1-perceptron">
                                <img src="../img/perceptron.jpeg" 
                                     alt="Mark1, le premier perceptron du monde" 
                                     title="Mark1, le premier perceptron du monde">
                                <figcaption>Mark1, le premier perceptron du monde</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h3>Structure d'un perceptron</h3>
                        <p>
                            Voici comment un neurone est schématiquement représenté dans le fonctionnement 
                            du cerveau humain.
                        </p>
                        <div class="figure-layout">
                            <figure id="human-neuron">
                                <img src="../img/neuron-human.png"
                                alt="Neurone humain" title="Neurone humain">
                                <figcaption>Neurone humain</figcaption>
                            </figure>
                        </div>
                        <p>
                            Un perceptron est une modélisation mathématique du fonctionnement d’un vrai neurone biologique
et qui imite grossièrement le fonctionnement de ce dernier. Il reçoit en paramètre des entrées X dont chaque
entrée est multipliée par un poids w. Toutes ces entrées sont alors sommées et passé dans une fonction dite
d’activation ou fonction de transfert. Cette fonction va comparer la somme pondérée à un à un certain seuil
b pour décider quel sera la sortie y du neurone.
                        </p>

                        <div class="figure-layout">
                            <figure id="perceptron">
                                <img src="">
                                <figcaption>Structure d’un neurone artificiel avec n entrées x i et une sortie y</figcaption>
                            </figure>
                        </div>
                        <div class="list-p">
                            <p>
                                Comme nous pouvons le constater, sa structure est composée de 6 parties :
                            </p>
                            <ul>
                                <li>Les entrées ($x_1, x_2, x_3, ..., x_n$)</li>
                                <li>Les poids ($w_1, w_2, w_3, ..., w_n$)</li>
                                <li>Une fonction somme ($\sum$)</li>
                                <li>Un biais ($w_0$)</li>
                                <li>Une fonction d'activation ($h$ qu'on note souvent par la notation grec $\phi$)</li>
                                <li>Une sortie ($y$)</li>
                            </ul>
                        </div>

                        <p>
                            Sur la précédente, les entrées sont les données que l’on fournit au perceptron afin qu’il calcule un résultat.
Ces données doivent être des mesures en lien avec le cas étudié. Dans le cadre de ce travail, les mesures
sont des pixels. Étant donné que les données utilisées peuvent varier, on peut avoir certains problèmes de
confiance avec les résultats obtenus
                        </p>
                        <p class="alinea">
                            En effet, si nous imaginons qu’une entrée peut avoir des valeurs qui varient entre 0et 1 et qu’une deuxième
entrée peut avoir des valeurs qui varient entre 0 et 3000, alors la deuxième entrée va avoir un impact
beaucoup plus important dans le résultat calculé. Afin de remédier à cela, il est impératif de normaliser les
données en entrée. Nous verrons plus tard que la normalisation peut avoir un impact sur l’apprentissage.
                        </p>
                        <div class="list-p">
                            <p>
                                La fonction de somme pondérée va faire la somme de la multiplication de chaque entrée par son poids
correspondant et va ajouter le biais, qui est une valeur constante. Avant de pouvoir utiliser le résultat, la
somme est passée dans une fonction d’activation.
                            </p>
                            <ul>
                                <li>La fonction <strong class="bold">Sigmoïde</strong> : Pour une valeur $x$ quelconque, $y$ reste entre 0 et 1. Ce cas est très utile
                                    lorsque nous avons besoin d’avoir un résultat continu. Elle est généralement notée par le symbole
                                    grec $\sigma$ représentée par la fonction $\mathbf\sigma(x) = \frac{1}{e^{-x}}<!--{1 \over exp^{-x}}-->$.
                                </li>
                                <li>
                                    La fonction <strong class="bold">$tanh$</strong> est également appelée tangente hyperbolique : Cette fonction ressemble à la
fonction Sigmoïde. La différence avec la fonction Sigmoïde est que la fonction $tanh$ produit un
résultat compris entre $-1$ et $1$.
                                </li>
                                <li>
                                    La fonction $ReLU$ (Unité linéaire rectifiée) : Cette fonction converge plus rapidement, optimise et
                                    produit la valeur souhaitée plus rapidement. C’est de loin la fonction d’activation la plus populaire
                                    utilisée dans les couches cachées sur un réseau de neurones.  La fonction $ReLU$ est interprétée par
                                    la formule : $f(x) = max(0,x)$.
                                </li>
                                <li>
                                    La fonction $Softmax$ : utilisé dans la couche de sortie, sur un réseau de neurones, car il réduit les
                                    dimensions et peut représenter une distribution catégorique.
                                </li>
                            </ul>
                            <p>(Vous trouverez encore plus d’autres fonctions sur <a href="https://fr.wikipedia.org/wiki/Fonction_d'activation#Liste_de_fonctions_d'activation_usuelles">Wikipédia</a>)</p>
                        </div>
                        <p>
                            La propagation d’un vecteur d’entrée ($x_i$) à travers un perceptron s’écrit :
                            $$p = \sum^{n}_{i=0} x_i w_i - b$$ (Avec $n$ le nombre d'entrées sur le perceptron)
                            $$y = \sigma(p)$$

                        </p>
                        <p>
                            Cela dit, un seul neurone ne suffit pas de faire des relations plus complexes. Les neurones on peut en
associer plein ensemble, les empiler pour créer des couches de neurones pour réussir à faire des fonctions
beaucoup plus compliquées. Et c’est cet ensemble de couche de neurones liées qu’on appelle un "Réseau
de neurones" (ou plutôt dire réseau de neurones artificiel).
                        </p>
                    </section>
                </section>
                
                <section class="main-section">
                    <h2>Apprentissage</h2>
                    <section>
                        <h3>Mode d'apprentissage</h3>
                        <p>
                            L’apprentissage d’une MLP peut être faite de plusieurs façons. On trouve l’apprentissage supervisé,
non supervisé, par renforcement, semi-supervisé.
                        </p>
                        <p>
Ici sera seulement explicité le mode d’apprentissage supervisé. Ce qui veut dire que les données
d’entraînement que vous fournissez à l’algorithme comportent les solutions désirées, appelées étiquettes
(en anglais, labels).
                        </p>
                        <div id="supervised">
                            <figure>
                                <img 
                                    src='../img/supervivee.png'
                                    alt='Mode d’apprentissage'
                                    title="Mode d’apprentissage"/>
                                <figcaption>
                                    Un jeu d’entraînement étiqueté pour un apprentissage 
                                    supervisé (ex. classification de spam)[A. Géron, Hands-On]
                                </figcaption>
                            </figure>
                        </div>
                    </section>
                    <section>
                        <h3>Apprentissage d'un perceptron</h3>
                        <p>
                            L’apprentissage consiste à envoyer des données au perceptron et à analyser le résultat. 
Ensuite, on indique au perceptron quel était le résultat attendu.<br>
Par exemple, si on obtient la valeur $0.3$ mais que la valeur attendue était de $1$, alors l’algorithme d’apprentissage 
du perceptron détecte qu’il a fait une erreur de $1–0.3=0.7$. Cette valeur peut être vue comme une fonction de coût et le perceptron doit s’adapter
(s’ajuster) afin de réduire le coût au minimum possible.
                        </p>
                        <p>
                            Pour cela, nous utilisons un algorithme appelé rétropropagation du gradient qui a pour but de
changer les poids de chaque connexion. Un poids ou poids synaptique est un coefficient numérique qui est
attribué à chacune des entrées d’un neurone, de manière aléatoire au début puis les poids sont adaptés au
fur et à mesure, et qui permet de pondérer celles-ci. (Wikipédia, 2017).
                        </p>
                        <div>
                            <figure id="perceptron-monocouche">
                                <img 
                                    src='../img/perceptron-apprentissage-dark.svg'
                                    alt="Renvoie de l'erreur vers l'arrière"
                                    title="Renvoie de l'erreur vers l'arrière" />
                                <figcaption>Renvoie de l'erreur vers l'arrière</figcaption>
                            </figure>
                        </div>
                        <p class="alinea">
                            Si toutes les entrées sont à 0, alors peu importe la valeur des poids donnés à chaque connexion, la valeur en sortie sera toujours 0. 
Dans ce cas, nous pouvons ajouter le biais qui nous permet de modifier la sortie afin que l’apprentissage se fasse correctement.
                        </p>
                        <section>
                            <h4>Le biais</h4>
                            <p>
                                Pour une bonne compréhension de ce qu’est le biais, nous allons considérer la fonction linéaire
                                $y=ax$. Et nous allons essayer de séparer deux ensembles de données (ici bleu et rouge).
                            </p>

                            <div>
                                <figure id="bias-1">
                                    <img 
                                        src='' 
                                        alt="Séparation de deux ensembles"
                                        title="Séparation de deux ensembles"/>
                                    <figcaption>Séparation de deux ensembles</figcaption>
                                </figure>
                            </div>

                            <div>
                                <p>
                                    Et comme nous pouvons le constater, la fonction $y=ax$  sépare parfaitement les deux ensembles.
                                </p>
                                <figure id="bias-2">
                                    <img 
                                        src='../img/bias_2-dark.svg' 
                                        alt="Séparation de deux ensembles à l'aide d'une fonction linéaire"
                                        title="Séparation de deux ensembles à l'aide d'une fonction linéaire"/>
                                    <figcaption>Séparation de deux ensembles à l'aide d'une fonction linéaire</figcaption>
                                </figure>
                            </div>
                            <div>
                                <p>
                                    Cependant, la limitation de ces fonctions se présente lorsque les ensembles de données sont moins faciles
    à séparer.<br>
    Sur la figure suivante, on remarque qu’aucune fonction ne permet de séparer les ensembles
    correctement avec une fonction linéaire. C’est de cette façon qu’on introduit le <strong class="bold">biais</strong>.
                                </p>
                                <figure id="bias-3">
                                   <img 
                                        src=""
                                        alt="Séparation impossible à l'aide une fonction linéaire"
                                        title="Séparation impossible à l'aide une fonction linéaire"/>
                                   <figcaption>Séparation impossible à l'aide une fonction linéaire</figcaption> 
                                </figure>
                            </div>
                            <p>
                                Le biais va permettre de rendre la séparations d'ensembles facile en permettant de mieux ajuster 
                                les limites entres les ensembles. Elle permet, en effet, d'avoir la possibilité de déplacer les droites
                                séparateurs sur les axes. Dans ce cas, on a pas forcement une droite qui passe par l'origine. Et c'est effet qui va permettre de trouver 
                                les limites entres les ensembles.
                            </p>
                            <div> 
                                <p>
                                    Sur la figure suivante, la fonction utilisée est $y = ax + b$ (<span class="italic">une fonction affine</span>)
                                    où $+b$ correspond au biais. Grâce à ce biais, nous avons pu trouver la bonne droite $𝑦=−0.3+9$ : une fonction affine 
                                     qui sépare bien les ensembles.  
                                </p>
                                <figure id="bias-4">
                                    <img 
                                        src=""
                                        alt="Séparation de deux ensemble à l'aide d'une fonction affine"
                                        title="Séparation de deux ensemble à l'aide d'une fonction affine"/>
                                        <figcaption>Séparation de deux ensemble à l'aide d'une fonction affine</figcaption>
                                </figure>
                            </div>
                            
                        </section>
                    </section>
                    <section>
                        <h3>Réseau de neurones monocouche</h3>
                        <div class="flex-bloc">
                            <div>
                                <p>Le réseau le plus simple est <strong class="bold"> Le perceptron monocouche.</strong></p>
                                <ul>
                                    <li>
                                        Il possède $N$ informations en entrées.
                                    </li>
                                    <li>
                                        Il est composé de $p$ neurones, que l’on représente généralement
alignés verticalement ou horizontalement.
                                    </li>
                                    <li>
                                        Chacun des $p$ neurones est connecté aux $N$ informations d’entrées.
                                    </li>
                                    <li>La sortie de chacun des $p$ neurones est une est une sortie du réseau.</li>
                                </ul>
                            </div>
                            <p>
                                Une utilisation courante est que chaque neurone de la couche représente une
<span class="underline">classe</span>. Pour un exemple $X$ donné, on obtient la classe de cet exemple en
prenant la plus grande des $p$ sorties
                            </p>
                            <div>
                                <figure  id='monocouche'>
                                    <img 
                                        src="../img/monocouche-dark.svg" 
                                        alt="Réseau de neurones monocouche"
                                        title="Réseau de neurone monocouche">
                                    <figcaption>Réseau de neurones monocouche</figcaption>
                                </figure>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Réseau de neurones multicouche (MLP pour Multi Layer Perceptron)</h3>
                        <p>
                            Dans ce réseau, les neurones de la première couche reçoivent toutes les informations entrées, ceux de la
deuxième reçoivent toutes les sorties des neurones de la première couche, et ainsi de suite jusqu’aux neurones
de sortie.
                        </p>

                        <div class="">

                        </div>
                    </section>
                </section>
                
                <footer>
                    <p>Coded by 
                        <a href="https://github.com/faouziMohamed/" target="_blank" title="Facebook account">
                            FAOUZI MOHAMED
                        </a>
                    </p>
                </footer>
            </article>
        </main>
        <a href="#top" class="to-top"></a>
        <script type="text/javascript" src="../js/switch.js"></script>
        <script type="text/javascript" src="../js/nav.js"></script>
        <script type="text/javascript" src="../js/body.js"></script>
        <noscript >
            <div id="noscript-layout">
                <p id="no-script-before-main">
                    <i class="fa fa-warning"></i>
                    La page web fonctionne bien avec javascript activé
                </p>        
            </div>
        </noscript>
    </body>


</html>